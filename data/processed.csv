,key,title,authors,abstract,year_published,tags
0,rayyan-354359269,An experience of automated assessment in a large-scale introduction programming course,"Zampirolli F.A., Borovina Josko J.M., Venero M.L.F., Kobayashi G., Fraga F.J., Goya D., Savegnago H.R.","The 2020 pandemic imposed new demands on teaching practices to support student's distance learning process. In this context, automated assessment (AA) is a pivotal resource that offers immediate and automatic feedback on students' programming tasks. Although the literature provides several contributions regarding AA of Programming Exercises (PEs), very few works discuss the automatic generation of personalized PE. This study reports our experience in applying a new proposal for AA-PE in an Introduction to Programming (IP) course for a large group of students. This proposal's key feature is the ability to apply AA-PE and parameterized unified exams to different programming languages by using the open-source tools MCTest and Moodle (with virtual programming lab [VPL] plugin). During the first quarter of 2019, teachers of 19 of 44 IP-FF (face-to-face) classes embraced our approach as a component in their pedagogical intervention. These classes achieved a higher pass rate (67.5%) than those that did not adopt our AA solution (59.1%), whereas the standard deviation was quite the same (22.5% and 21.3%, respectively). Additionally, preliminary results revealed a strong linear correlation (r =.93) between the pass rate and the average grade of the AA-PE. In IP-BL (blended learning), two classes used our method in the exams, with 171 students and a pass rate of 70.4%. These results corroborate previous works that continuous assessment combined with immediate feedback can contribute to students' learning process. © 2021 Wiley Periodicals LLC",2021,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:with_without', 'feedback:test_output', 'interaction:unknown', 'language:python', 'skill:correctness', 'technique:pre_defined_questions', 'technique:test_cases', 'tool:mctest', 'tool:moodle', 'tool:vpl', 'type:description', 'type:evaluation', 'type:experience_report']"
1,rayyan-354359270,Semiautomatic generation and assessment of Java exercises in engineering education,"Insa D., Pérez S., Silva J., Tamarit S.","Continuous assessment is essential in education. It should be an integral part of education that provides immediate feedback to students. Unfortunately, the assessment of programming source code is still a hand-operated and error-prone task, and can take weeks before the student gets feedback. This study presents a semiautomatic code assessment method able to automatically apply black-box assessment (which relies on the comparison of input-output pairs) and white-box assessment (which relies on checking different source code properties). The method proposed is a general-purpose assessment system that was originally designated to be used in engineering education, but that can be used in other educational contexts to assist the assessment of any Java programming assignments or exams. The main advantage of this system is that the assessment made is quicker, exhaustive, and objective; and it does not produce false positives. After the application of this method over two years in several real university courses, we have released a public and free implementation. An empirical evaluation of this system estimates that the amount of assessment work automatically done by the tool is over 48%. Additionally, the system has been used to measure the average subjective influence (i.e., assessment errors) introduced by teachers when they assess exams manually. © 2020 Wiley Periodicals LLC",2021,"['approach:fully_automated', 'approach:semi_automatic', 'data:internal_exams', 'data_available:on_request', 'evaluation:auto_grading', 'evaluation:compared_to_human', 'feedback:personalised_feedback', 'feedback:rule_failure', 'interaction:unknown', 'language:java', 'skill:correctness', 'skill:maintainability', 'technique:code_repair_for_feedback', 'technique:dsl_for_exercise_gen', 'technique:dsl_rules', 'technique:model_solution_req', 'technique:pattern_matching', 'technique:test_cases', 'tool:javassess', 'type:description', 'type:evaluation']"
2,rayyan-354359271,A tool for evaluating computer programs from students,"Vaneck Q., Colart T., Frénay B., Vanderose B.","Computer science studies are more and more popular, and teachers must face and adapt to the increasing number of students. Whereas small groups allowed more interactions between teachers and students, the resulting overcrowding takes away closeness and forces teachers to spend less time with each student. Therefore, the student can quickly feel submerged and helpless against the difficulty of the course. This paper proposes a solution that aims to reduce drop-out in programming courses. It offers an accurate feedback on the quality of students' Python code to deepen their understanding, together with a playful interface to boost their interest in programming. This solution developed under the name of ""METAssistant""has two objectives. It allows students to use it to evaluate their programs and to get an accurate feedback, and teachers to have an overview of the understanding of the matter by their students. © 2021 ACM.",2021,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:gamified', 'feedback:personalised_feedback', 'interaction:unknown', 'language:python', 'skill:code_quality', 'skill:maintainability', 'skill:readability', 'technique:code_metrics', 'technique:metrics', 'technique:program_repair', 'technique:static_analysis', 'tool:pylint', 'type:description']"
3,rayyan-354359272,Virtual Teaching Assistant for Grading Programming Assignments: Non-dichotomous Pattern based Program Output Matching and Partial Grading Approach,"Chou C.-Y., Chen Y.-J.","This study proposes an automated programming assessment system with a virtual teaching assistant (VTA) grading mechanism to automatically assess and grade functional correctness of students' programming assignments by using a non-dichotomous pattern-based program output matching and partial grading approach. The VTA grading mechanism matches student program output with location-free or location-specific patterns to release the strict specification of program output and offer partial marks on programs with partial functional correctness to let students know they are on the right track. The VTA grading results were applied as immediate formative assessments to help students refine their programs. The results of an evaluation showed that the VTA grading had high accuracy and helped students refine their programs, particularly for complex programs. In addition, most students expressed a positive attitude toward the VTA grading mechanism. © 2021 IEEE.",2021,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_grading', 'evaluation:compared_to_human', 'feedback:test_output', 'interaction:multiple', 'language:unknown_language', 'skill:correctness', 'technique:output_matching', 'technique:pattern_matching', 'technique:test_cases', 'tool:vta', 'type:description', 'type:evaluation']"
4,rayyan-354359273,Analysis of an automatic grading system within first year Computer Science programming modules,"Hegarty-Kelly E., Mooney D.A.","Reliable and pedagogically sound automated feedback and grading systems are highly coveted by educators. Automatic grading systems are useful for ensuring equity of grading of student submissions to assignments and providing timely feedback on the work. Many of these systems test submissions to assignments based on test cases and the outputs that they achieve, while others use unit tests to check the submissions. The approach presented in this paper checks submissions based on test cases but also analyses what the students actually wrote in their code. Assignment questions are constructed based around the concepts that the student are currently learning in lectures, and the patterns searched for in their submissions are based on these concepts. In this paper we show how to implement this approach effectively. We analyse the use of an automatic grading system within first year Computer Science programming modules and show that the system is straightforward to use and suited for novice programmers, while providing automatic grading and feedback. Evaluation received from students, demonstrators and lecturers show the system is extremely beneficial. The evaluation shows that such systems allow demonstrators more time to assist students during labs. Lecturers can also provide instant feedback to students while keeping track of their progress and identifying where the gaps in students' knowledge are. © 2021 Owner/Author.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_grading', 'evaluation:student_survey', 'evaluation:user_study', 'feedback:test_output', 'interaction:unknown', 'language:java', 'skill:code_quality', 'skill:correctness', 'technique:dsl_rules', 'technique:pattern_matching', 'technique:test_cases', 'technique:unit_testing', 'tool:mule', 'tool:vpl', 'type:description', 'type:evaluation']"
5,rayyan-354359274,Evaluating Control-Flow Graph Similarity for Grading Programming Exercises,"Sendjaja K., Rukmono S.A., Perdana R.S.","Programming has become a fundamental skill in the current digital era. A formal programming course relies on an autograder to score student works. However, the usual black-box method only compares the output instead of adequately examining the code structure. As such, another method is required to measure the structure of the student submission code to give fairer scores. In this paper, an experiment is conducted using a control-flow graph (CFG) comparison algorithm to measure the similarity between student submission code and reference code from the instructor, followed by an analysis of the results obtained from the experiment. The comparison was made using the Hu Algorithm [1], based on previous CFG similarity measurements presented by Chan and Collberg [2]. Through the experiment and analysis process, it is concluded that the CFG comparison method implemented in this research is better applied to boost students who got low scores due to minor mistakes rather than be applied to the entire student submissions as the primary scoring algorithm. © 2021 IEEE.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'feedback:none', 'interaction:unknown', 'language:unknown_language', 'skill:correctness', 'technique:control_flow_graph', 'technique:model_solution_closeness', 'technique:model_solution_req', 'technique:pattern_matching', 'tool:none', 'type:description', 'type:evaluation']"
6,rayyan-354359276,Model-based automatic grading of object-oriented programming assignments,Le D.M.,"Automatic grading of object-oriented programming (OOP) assignments is an important problem from practical, theoretical, and educational viewpoints. Apart from computing a specific grade, an effective grading method needs to provide systematic feedback comments to both the design and code elements. Existing works have proposed grading methods that make various assumptions about the design. However, none of these methods have considered using a design language for the program model. A challenge here is to use a language that eases learning and application in program design. In this paper, we propose a novel grading method, named OOPGRADER, which automatically grades OOP assignments constructed based on an essentially detailed program model. This model is defined in an embedded design language, which directly uses the annotation feature of OOP language to express the essential design rules. We explain how programming assignments can be designed with this language and propose a program checker and a grader for both student and teacher to use in working on the assignments and in grading them. We implement these components in a tool and develop an Eclipse plugin for this tool. We evaluate our method using a combination of qualitative and quantitative techniques. The main result is that our method helps students effectively learn to program through detailed feedback comments based on a program model. The tool is adaptable, has a good performance, and helps enhance the productivity of both students and teachers through IDE integration. © 2021 Wiley Periodicals LLC",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:on_request', 'evaluation:auto_graders', 'evaluation:auto_grading', 'feedback:personalised_feedback', 'feedback:rule_failure', 'feedback:unit_testing', 'interaction:multiple', 'language:java', 'skill:correctness', 'skill:maintainability', 'skill:oop', 'technique:dsl', 'technique:dsl_rules', 'technique:model_based', 'technique:model_solution_req', 'technique:test_cases', 'tool:oopgrader', 'type:description', 'type:evaluation']"
7,rayyan-354359277,An interactive feedback system for grammar development (tool paper),"Barraball C., Raselimo M., Fischer B.","We describe gtutr, an interactive feedback system designed to assist students in developing context-free grammars and corresponding ANTLR parsers. It intelligently controls students' access to a large test suite for the target language. After each submission, gtutr analyzes any failing tests and uses the Needleman-Wunsch sequence alignment algorithm over the tests' rule traces to identify and eliminate similar failing tests. This reduces the redundancy in the feedback given to the students and prevents them from being overloaded. gtutr uses simple gamification to encourage independent problem solving by students: it gives as little information as possible, and students need to prompt the system for further details such as failing tests similar to or different from already seen tests, or even for hints about rules that are the most likely to contain faults. It tracks the students' information requests and uses this to attenuate marks following an instructor-set penalty schema. The system also visualizes test outcomes over multiple submissions, helping students to keep track of the effects of their changes as their grammar development progresses. © 2020 ACM.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:on_request', 'evaluation:grade_correlation', 'evaluation:student_survey', 'feedback:personalised_feedback', 'feedback:unit_testing', 'interaction:multiple', 'language:java', 'skill:correctness', 'skill:teaching_context_free_grammar', 'technique:test_cases', 'technique:trace_examination', 'technique:unit_testing', 'tool:oopgrader', 'type:description', 'type:evaluation']"
8,rayyan-354359278,Combining dynamic and static analysis for automated grading SQL statements,"Wang J., Zhao Y., Tang Z., Xing Z.","Learning and teaching Structured Query Language (SQL) is a challenge that is widely recognized within the computer science education community. Programming ac-tivities are considered to be the most important aspect of the learning process. Manually grading of SQL statements, however, is often regarded as a tedious and time-consuming ordeal. In this paper we propose an automated grading approach for SQL statements, which combines dynamic and static analysis. The proposed approach first identified the correct statements from the student SQL statements, and then grades the incorrect statements based on the similarity between the statements and the correct statements. Empir-ical evaluation of the proposal was carried out on a dataset consisting of 255 manually graded pairs of statements. Experimental results show that our approach is superior to three state-of-the-art grading approaches. Our study found that each grading approach has its particular advantages and disadvantages, and the combination of different grading approaches can help to improve the quality of grading results. © 2020, Taiwan Ubiquitous Information. All rights reserved.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:True', 'evaluation:compared_to_human', 'evaluation:grading_accuracy', 'feedback:none', 'interaction:unknown', 'language:sql', 'skill:correctness', 'technique:dynamic_analysis', 'technique:model_solution_closeness', 'technique:model_solution_req', 'technique:pattern_matching', 'technique:static_analysis', 'technique:unit_testing', 'tool:none', 'type:description', 'type:evaluation']"
9,rayyan-354359279,Assisted learning of C programming through automated program repair and feed-back generation,"Arifi S.M., Abbou R.B., Zahi A.","Programming courses are among all the current academic curricula for engineering studies. Unfortunately, students often face difficulties already on the basic concepts. Both students and teachers believe that practical sessions and guided learning lead to good outcomes. On the other hand, it is virtually difficult considering the number of students enrolled on programming courses. This paper presents an automated assessment system for programming assignments, based on two different methods: static and dynamic analysis. The presented system aims at providing the student with an ongoing and various feedback delivered according to the category and the recurrence of errors. The system imbeds an automated error repairing feature for the purposes of insuring the assessment process achievement. It operates if the student fails to submit a correct program despite the feed-back provided by the system. In such cases, the system uses a penalty mechanism, customized by the teacher to grade the student.s program. Testing the presented automated system, through assessing real students. assignments, showed promising results compared to manual assessment. Copyright © 2020 Institute of Advanced Engineering and Science. All rights reserved.",2020,"['approach:fully_automated', 'data:internal_exams', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:grading_accuracy', 'feedback:augmented_compiler_errors', 'feedback:pre_defined_feedback', 'interaction:multiple', 'language:C', 'skill:correctness', 'technique:dynamic_analysis', 'technique:model_solution_closeness', 'technique:output_matching', 'technique:pattern_matching', 'technique:program_repair', 'technique:static_analysis', 'technique:test_cases', 'tool:claas', 'type:description', 'type:evaluation']"
10,rayyan-354359280,Syntactic and Semantic Analysis for Extended Feedback on Computer Graphics Assignments,"Andujar C., Vijulie C.R., Vinacua A., Santos B.S., Alford G.","Modern computer graphics courses require students to complete assignments involving computer programming. The evaluation of student programs, either by the student (self-Assessment) or by the instructors (grading) can take a considerable amount of time and does not scale well with large groups. Interactive judges giving a pass/fail verdict do constitute a scalable solution, but they only provide feedback on output correctness. In this article, we present a tool to provide extensive feedback on student submissions. The feedback is based both on checking the output against test sets, as well as on syntactic and semantic analysis of the code. These analyses are performed through a set of code features and instructor-defined rubrics. The tool is built with Python and supports shader programs written in GLSL. Our experiments demonstrate that the tool provides extensive feedback that can be useful to support self-Assessment, facilitate grading, and identify frequent programming mistakes. © 1981-2012 IEEE.",2020,"['approach:semi_automatic', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:test_output', 'interaction:unknown', 'language:glsl', 'skill:code_quality', 'skill:correctness', 'skill:graphics', 'technique:dsl_rules', 'technique:test_cases', 'tool:antlr', 'type:description']"
11,rayyan-354359282,Computing with CodeRunner at Coventry University Automated summative assessment of Python and C++ code,"Croft D., England M.",CodeRunner is a free open-source Moodle plugin for automatically marking student code. We describe our experience using CodeRunner for summative assessment in our first year undergraduate programming curriculum at Coventry University. We use it to assess both Python3 and C++14 code (CodeRunner supports other languages also). We give examples of our questions and report on how key metrics have changed following its use at Coventry. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.,2020,"['approach:fully_automated', 'data:internal_exams', 'data_available:False', 'evaluation:with_without', 'feedback:test_output', 'interaction:unknown', 'language:c++', 'language:python', 'skill:correctness', 'skill:readability', 'technique:dsl_rules', 'technique:test_cases', 'technique:unit_testing', 'tool:coderunner', 'type:experience_report']"
12,rayyan-354359284,Using an interactive software tool for the formative and summative evaluation in a computer programming course: An experience report,"Restrepo-Calle F., Ramírez-Echeverry J.J., González F.A.","This article describes the use of an automatic tool that supports both formative and summative evaluation, in an introductory computer programming course. The aim was to test the tool in a real scenario and to assess its potential benefits. The participants were 56 students in an engineering school. Different programming tasks were solved by students with the support of the tool. It provides summative evaluation using automatic grading, and formative evaluation through real-time checking of good coding practices, code execution visualisation, testing code with custom inputs, informative feedback on failed test cases, and student performance statistics for self-monitoring the learning process. Results of the experience show the students considerably used the tool, denoting high engagement to achieve the learning goals of the tasks. The students perceived the tool was highly useful to debug programs and carry out frequent practice of programming, thanks to the automatic feedback and test of solutions for programming assignments. Results offer evidence of the importance of providing comprehensive feedback to facilitate students finding mistakes and building knowledge about how to proceed in case of errors. Moreover, the tool shows some potential to help students developing learning strategies, e.g. metacognitive awareness and improving problem-solving skills. © 2020 World Institute for Engineering and Technology Education. All rights reserved.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data:internal_exams', 'data_available:False', 'evaluation:analytics', 'evaluation:student_survey', 'feedback:unit_testing', 'interaction:multiple', 'language:C', 'language:c++', 'language:java', 'language:python', 'skill:code_quality', 'skill:readability', 'technique:static_analysis', 'technique:unit_testing', 'tool:uncode', 'type:experience_report']"
13,rayyan-354359285,An Intelligent Online Judge System for Programming Training,"Dong Y., Hou J., Lu X.","Online judge (OJ) systems are becoming increasingly popular in various applications such as programming training, competitive programming contests and even employee recruitment, mainly due to their ability of automatic evaluation of code submissions. In higher education, OJ systems have been extensively used in programming courses because the automatic evaluation feature can drastically reduce the grading workload of instructors and teaching assistants and thereby makes the class size scalable. However, in our teaching we feel that existing OJ systems should improve their ability on giving feedback to students and teachers, especially on code errors and knowledge states. The lack of such automatic feedback increases teachers’ involvement and thus prevents college programming training from being more scalable. To tackle this challenge, we leverage historical student data obtained from our OJ system and implement two automated functions, namely, code error prediction and student knowledge tracing, using machine learning models. We demonstrate how students and teachers may benefit from the adoption of these two functions during programming training. © 2020, Springer Nature Switzerland AG.",2020,"['approach:oj', 'data:internal_assignments', 'data_available:False', 'evaluation:none', 'feedback:machine_learning', 'interaction:unknown', 'language:C', 'skill:correctness', 'technique:error_prediction', 'technique:machine_learning', 'technique:ml', 'technique:test_cases', 'technique:unit_testing', 'tool:code2vec', 'type:demo_paper']"
14,rayyan-354359287,Teaching programming at scale,"Kaplan A., Keim J., Schneider Y.R., Walter M., Werle D., Koziolek A., Reussner R.","Teaching programming is a difficult task and there are many different challenges teachers face. Beyond considerations about the right choice of teaching content and presentation in lectures, scaling practical parts of courses and the examination and grading to course sizes of around 1,000 students is particularly challenging. We believe programming is a skill that needs to be trained practically, which creates additional challenges, especially at this scale. In this paper, we outline learning goals for our undergraduate programming course and the structure for the course we derived from these goals. We report on the challenges we see when teaching programming at scale and how we try to overcome them. For example, one central challenge is how to grade a high number of students in a good, transparent, and efficient way. We report on our approach that includes automated tests as well as tool support for manual code review. Over the years, we experienced different issues and learned valuable lessons. We present corresponding key takeaways that we derived from our experiences. Copyright © 2020 for this paper by its authors.",2020,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:none', 'feedback:test_output', 'interaction:single', 'language:java', 'skill:code_quality', 'skill:correctness', 'skill:readability', 'technique:style_check', 'technique:test_cases', 'technique:unit_testing', 'tool:checkstyle', 'type:experience_report']"
15,rayyan-354359290,Fast test suite-driven model-based fault localisation with application to pinpointing defects in student programs,"Birch G., Fischer B., Poppleton M.","Fault localisation, i.e. the identification of program locations that cause errors, takes significant effort and cost. We describe a fast model-based fault localisation algorithm that, given a test suite, uses symbolic execution methods to fully automatically identify a small subset of program locations where genuine program repairs exist. Our algorithm iterates over failing test cases and collects locations where an assignment change can repair exhibited faulty behaviour. Our main contribution is an improved search through the test suite, reducing the effort for the symbolic execution of the models and leading to speed-ups of more than two orders of magnitude over the previously published implementation by Griesmayer et al. We implemented our algorithm for C programs, using the KLEE symbolic execution engine, and demonstrate its effectiveness on the Siemens TCAS variants. Its performance is in line with recent alternative model-based fault localisation techniques, but narrows the location set further without rejecting any genuine repair locations where faults can be fixed by changing a single assignment. We also show how our tool can be used in an educational context to improve self-guided learning and accelerate assessment. We apply our algorithm to a large selection of actual student coursework submissions, providing precise localisation within a sub-second response time. We show this using small test suites, already provided in the coursework management system, and on expanded test suites, demonstrating the scalability of our approach. We also show compliance with test suites does not reliably grade a class of “almost-correct” submissions, which our tool highlights, as being close to the correct answer. Finally, we show an extension to our tool that extends our fast localisation results to a selection of student submissions that contain two faults. © 2017, The Author(s).",2017,"['approach:unclear', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_other_tools', 'feedback:unclear', 'interaction:unknown', 'language:C', 'skill:correctness', 'technique:fault_localisation', 'technique:program_repair', 'technique:program_verification', 'technique:test_cases', 'tool:none', 'type:description', 'type:evaluation']"
16,rayyan-354359291,A parser-based tool to assist instructors in grading computer graphics assignments,"Andujar C., Vijulie C.R., Vinacua A.","Although online e-learning environments are increasingly used in university courses, manual assessment still dominates the way students are graded. Interactive judges providing a pass/fail verdict based on test sets are valuable tools both for learning and assessment, but still rely on human review of the code for output-independent issues such as readability and efficiency. In this paper we present a tool to assist instructors in grading programming exercises in Computer Graphics (CG) courses. In contrast to other grading solutions, assessment is based both on checking the output against test sets, and through a set of instructor-defined rubrics based on syntax analysis of the source code. Our current prototype runs in Python and supports the assessment of shaders written in GLSL language. We tested the tool in a CG course involving more than one hundred Computer Science students per year. Our first experiments show the tool can be useful to support both self-assessment and grading, as well as detecting grading mistakes through anomaly detection techniques based on features extracted from the syntax analysis. © 2019 The Author(s) Eurographics Proceedings © 2019 The Eurographics Association.",2019,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:manual_personalised_feedback', 'interaction:unknown', 'language:c++', 'language:glsl', 'skill:code_quality', 'skill:correctness', 'technique:dsl_rules', 'technique:model_solution_req', 'technique:test_cases', 'technique:unit_testing', 'tool:none', 'type:description', 'type:evaluation']"
17,rayyan-354359293,Improving Programming Education Quality with Automatic Grading System,"Cai Y.-Z., Tsai M.-H.","As the rapid growth of information technology, the demand for proficiency in software programming skyrockets. Compared to teaching with slides traditionally, hands-on programming training is more beneficial and practical. However, it is exhausting and time-consuming for educators to grade all assignments in person. Besides, students may not get feedback immediately to correct their wrong conceptions. Therefore, an automatic grading system is required to grade and send feedback to students. Based on an existing continuous integration system, which checks whether new programs behave as expected, we develop a set of course management tools and deploy an automatic grading system in this paper. Our system requires a server to run and test the programs. However, the server is susceptible to being compromised by hackers. Therefore, how we protect sensitive data and prevent malicious network traffic are demonstrated in this paper as well. The tools were applied in an Android application development course with 140 students enrolled. Around 72% of the students indicate the automatic grading system is beneficial to their learning. © Springer Nature Switzerland AG 2019.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_grading', 'evaluation:student_survey', 'feedback:test_output', 'interaction:unknown', 'language:android', 'skill:correctness', 'technique:ci', 'technique:test_cases', 'technique:unit_testing', 'tool:travis', 'type:description', 'type:evaluation']"
18,rayyan-354359294,Grading Programs Based on Hybrid Analysis,"Wang Z., Xu L.","Grading programming assignments often take a lot of time and effort, so there is a growing need for automatic scoring systems. The existing program scoring system gives scores mainly by executing test cases. It cannot identify homework plagiarism or give grades according to steps. Therefore, this paper proposes an automatic scoring technique for Python programming assignments based on hybrid program analysis. Our scoring tool Paprika combines dynamic analysis and static analysis methods. Based on the Python abstract tree, it uses static analysis to realize the detection of clones and code style assessment and uses dynamic analysis to score with running test cases. It has a good tolerance for various formats of output of the code. The experiment shows that on the one hand, Paprika can reduce the burden of teachers and teaching assistants; on the other hand, it can accurately and objectively give grades and detailed feedback, which is helpful for students to improve learning efficiency and programming level. © 2019, Springer Nature Switzerland AG.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data:leetcode', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:manual_grading', 'feedback:plagiarism', 'feedback:style_check', 'interaction:single', 'language:python', 'skill:code_quality', 'skill:correctness', 'technique:dynamic_analysis', 'technique:static_analysis', 'technique:test_cases', 'technique:unit_testing', 'tool:deckard', 'type:description', 'type:evaluation']"
19,rayyan-354359295,Convolutional neural network applied to code assignment grading,"De Souza F.R., De Assis Zampirolli F., Kobayashi G.","Thousands of students have their assignments evaluated by their teachers every day around the world while developing their studies in any branch of science. A fair evaluation of their schoolwork is a very challenging task. Here we present a method for validating the grades attributed by professors to students programming exercises in an undergraduate introductory course in computer programming. We collected 938 final exam exercises in Java Language developed during this course, evaluated by different professors, and trained a convolutional neural network over those assignments. First, we submit their codes to a cleaning process (by removing comments and anonymizing variables). Next, we generated an embedding representation of each source code produced by students. Finally, this representation is taken as the input of the neural network which classifies each label (corresponding to the possible grades A, B, C, D or F). An independent neural network is trained with source code solutions corresponding to each assignment. We obtained an average accuracy of 74.9% in a 10−fold cross validation for each grade. We believe that this method can be used to validate the grading process made by professors in order to detect errors that might happen during this process. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",2019,"['approach:semi_automatic', 'data:internal_exams', 'data_available:False', 'evaluation:grading_accuracy', 'feedback:errors', 'interaction:single', 'language:java', 'skill:correctness', 'technique:machine_learning', 'technique:ml', 'tool:mctest', 'type:description', 'type:evaluation']"
20,rayyan-354359296,Algorithm for adaptive learning process and improving learners’ skills in Java programming language,"Gavrilović N., Arsić A., Domazet D., Mishra A.","Adaptive approaches within distance learning systems enable adapting teaching process to the needs of each learner during the learning process. This paper presents an algorithm for creating an adaptive learning process that provides knowledge and skills improvement for learners in the Java programming language. Also, it presents the application of the tool that checks the learner's knowledge through solving practical tasks from the Java programming language. The adaptive learning process in this work leads the learner through teaching materials and practical tasks where the acquired knowledge is required to be applied. Also, the algorithm, based on the measurement of knowledge and time spent on a particular part of the learning process with detailed feedback and the demonstration of observed deficiencies, directs the learner to teaching materials that allow improving the demonstrated knowledge. Teaching materials are conceived as learning objects and, as such, allow for the application of adaptive approach. An analysis of the effectiveness of the algorithm and tool for practical knowledge testing from the Java programming language was done with a test group of learners who gave their opinions and grades. © 2018 Wiley Periodicals, Inc.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'evaluation:teacher_survey', 'feedback:unclear', 'interaction:unknown', 'language:java', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:none', 'type:description', 'type:evaluation']"
21,rayyan-354359297,Automatic assessment of Java code,"Insa D., Silva J.","Assessment is an integral part of education often used to evaluate students, but also to provide them with feedback. It is essential to ensure that assessment is fair, objective, and equally applied to all students. This holds, for instance, in multiple-choice tests, but, unfortunately, it is not ensured in the assessment of source code, which is still a manual and error-prone task. In this paper, we present JavAssess, a Java library with an API composed of around 200 methods to automatically inspect, test, mark, and correct Java code. It can be used to produce both black-box (based on output comparison) and white-box (based on the internal properties of the code) assessment tools. This means that it allows for marking the code even if it is only partially correct. We describe the library, how to use it, and we provide a complete example to automatically mark and correct a student's code. We also report the use of this system in a real university context to compare manual and automatic assessment in university courses. The study reports the average error in the marks produced by teachers when assessing source code manually, and it shows that the system automatically assesses around 50% of the work. © 2018 Elsevier Ltd",2018,"['approach:fully_automated', 'data:internal_exams', 'data_available:False', 'evaluation:auto_grading', 'evaluation:compared_to_human', 'evaluation:manual_grading', 'evaluation:student_survey', 'feedback:test_output', 'interaction:unknown', 'language:java', 'skill:code_quality', 'skill:correctness', 'technique:code_metrics', 'technique:dsl_rules', 'technique:model_solution_req', 'technique:reflection', 'tool:javassess', 'type:description']"
22,rayyan-354359298,Grading at scale in earsketch,"Sarwate A., Brunch C., Freeman J., Siva S.","This paper explores some of the challenges posed by automated grading of programming assignments in a STEAM (Science, Technology, Engineering, Art, and Math) based curriculum, as well as how these challenges are addressed in the automatic grading processes used in EarSketch, a music-based educational programming environment developed at Georgia Tech. This work-inprogress paper reviews common strategies for grading programming assignments at scale and discusses how they are combined in EarSketch to evaluate open ended STEAM-focused assignments. © 2017 Association for Computing Machinery. All rights reserved.",2017,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:unclear', 'interaction:unknown', 'language:javascript', 'language:python', 'skill:complexity', 'skill:correctness', 'technique:code_metrics', 'technique:dsl_rules', 'technique:model_solution_closeness', 'technique:model_solution_req', 'technique:pattern_matching', 'tool:earsketch', 'type:description']"
23,rayyan-354359299,"Search, align, and repair: Data-driven feedback generation for introductory programming exercises","Wang K., Singh R., Su Z.","This paper introduces the ""Search, Align, and Repair"" data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use. © 2018 ACM.",2018,"['approach:fully_automated', 'data:microsoft_dev204_1x', 'data_available:on_request', 'evaluation:analytics', 'evaluation:feedback', 'evaluation:with_without', 'feedback:program_repair', 'interaction:unknown', 'language:c#', 'skill:correctness', 'technique:cluster', 'technique:code_repair_for_feedback', 'technique:data_driven', 'technique:model_solution_closeness', 'technique:program_repair', 'tool:sarfgen', 'type:description', 'type:evaluation']"
24,rayyan-354359300,Investigating the feasibility of automatic assessment of programming tasks,"Liebenberg J., Pieterse V.","Aim/Purpose The aims of this study were to investigate the feasibility of automatic assessment of programming tasks and to compare manual assessment with automatic as-sessment in terms of the effect of the different assessment methods on the marks of the students. Background Manual assessment of programs written by students can be tedious. The assis-tance of automatic assessment methods might possibly assist in reducing the assessment burden, but there may be drawbacks diminishing the benefits of ap-plying automatic assessment. The paper reports on the experience of a lecturer trying to introduce automated grading. Students' solutions to a practical Java pro-gramming test were assessed both manually and automatically and the lecturer tied the experience to the unified theory of acceptance and use of technology (UTAUT). Methodology The participants were 226 first-year students registered for a Java programming course. Of the tests the participants submitted, 214 were assessed both manually and automatically. Various statistical methods were used to compare the manual assessment of student's solutions with the automatic assessment of the same solutions. A detailed investigation of reasons for differences was also carried out. A further data collection method was the lecturer's reflection on the feasibility of automatic assessment of programming tasks based on the UTAUT. Contribution This study enhances the knowledge regarding benefits and drawbacks of auto-matic assessment of students' programming tasks. The research contributes to the UTAUT by applying it in a context where it has hardly been used. Further-more, the study is a confirmation of previous work stating that automatic assess-ment may be less reliable for students with lower marks, but more trustworthy for the high achieving students. Findings An automatic assessment tool verifying functional correctness might be feasible for assessment of programs written during practical lab sessions but could be less useful for practical tests and exams where functional, conceptual and structural correctness should be evaluated. In addition, the researchers found that automatic assessment seemed to be more suitable for assessing high achieving students. Recommendations for Practitioners This paper makes it clear that lecturers should know what assessment goals they want to achieve. The appropriate method of assessment should be chosen wisely. In addition, practitioners should be aware of the drawbacks of automatic assess-ment before choosing it. Recommendation for Researchers This work serves as an example of how researchers can apply the UTAUT theory when conducting qualitative research in different contexts. Impact on Society The study would be of interest to lecturers considering automated assessment. The two assessments used in the study are typical of the way grading takes place in practice and may help lecturers understand what could happen if they switch from manual to automatic assessment. Future Research Investigate the feasibility of automatic assessment of students' programming tasks in a practical lab environment while accounting for structural, functional and conceptual assessment goals. © 2018.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:manual_grading', 'feedback:test_output', 'interaction:single', 'language:c#', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:fitchfork', 'type:experience_report']"
25,rayyan-354359301,Auto-marking system: A support tool for learning of programming,"Bakar M.A., Esa M.I., Jailani N., Mukhtar M., Latih R., Zin A.M.","Computer programming requires skills in designing algorithms, understanding syntax, writing programs, as well as the ability to correct errors in order to produce good programs. These skills can be developed through much practice on a continuous basis. The students' proficiency in programming is measured by the number of exercises that can be solved correctly within a specified period. From past observations, it is discovered that most students were able to solve the problems given during laboratory sessions. However, their performances did not carry over to laboratory tests. This situation points to the possibility that the students might not have performed adequate self-practice in preparing for laboratory tests. In a student-centered learning environment, fulfilling the notional learning hours is essential to ensure that students are prepared to take their subsequent classes. Based on a constructivist-learning framework, this article reports the development and evaluation of a prototype system to assist in the selflearning of programming. The online Auto-marking Programming Exercise System was developed based on the UVa Online Judge as a benchmark. The system can provide real-time feedback to students immediately after the students submit their programs. This instant feedback is an essential characteristic of the constructivist approach to learning. This will help students learn to programme in a useful way. The system is tested and evaluated for usability by selected users from among instructors and former students of computer programming course. © IJASEIT.",2018,"['approach:oj', 'data:none', 'data_available:False', 'evaluation:student_survey', 'feedback:unclear', 'interaction:multiple', 'language:language__agnostic', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:none', 'type:description', 'type:evaluation']"
26,rayyan-354359302,Automatic Assessment via Intelligent Analysis of Students’ Program Output Patterns,"Poon C.K., Wong T.-L., Tang C.M., Li J.K.L., Yu Y.T., Lee V.C.S.","Automatic assessment of computer programming exercises offers a number of benefits to both learners and educators, including timely and customised feedback, as well as saving of human effort in grading. However, due to the high variety of programs submitted by students, exact matching between the expected output and different output variants is undesirable and how to do the matching properly is a challenging and practical problem. Existing approaches to address this problem adopt various inexact matching strategies, but typically they are unscalable, incapable of expressing a diversity of program outputs, or require high level of expertise. In this paper, we propose Hierarchical Program Output Structure (HiPOS), which provides higher expressiveness and flexibility, to model the program output. Based on HiPOS, we design different levels of matching rules in the matching rule hierarchy to determine the admissible program output variants in a flexible and scalable manner. We conducted experiments and compare our approach of automatic assessment to human judgement. The results show that our proposed method achieved an accuracy of 0.8467 in determining the admissible program output variants. © 2018, Springer International Publishing AG, part of Springer Nature.",2018,"['approach:fully_automated', 'data:internal_tutorial', 'data_available:False', 'evaluation:compared_to_human', 'feedback:none', 'interaction:unknown', 'language:c++', 'skill:correctness', 'technique:matching_rules', 'technique:test_cases', 'technique:unit_testing', 'tool:hipos', 'type:description', 'type:evaluation']"
27,rayyan-354359303,TipsC: Tips and corrections for programming MOOCs,"Sharma S., Agarwal P., Mor P., Karkare A.","With MOOC sizes increasing every day, improving scalability and practicality of grading and tutoring of such courses is a worthwhile pursuit. To this end, we introduce TipsC. By analyzing a large number of correct submissions, TipsC can search for correct codes resembling a given incorrect solution. TipsC then suggests changes in the incorrect code to help the student fix logical errors. We evaluate the effectiveness of TipsC’s clustering algorithm on data collected from past offerings of an introductory programming course conducted at IIT Kanpur. The results show the weighted average variance of marks for clusters when similar submissions are grouped together is 47% less compared to the case when all programs are grouped together. © Springer International Publishing AG, part of Springer Nature 2018.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'feedback:suggest_similar', 'interaction:unknown', 'language:C', 'skill:correctness', 'technique:cluster', 'technique:code_repair_for_feedback', 'technique:pattern_matching', 'tool:tipsc', 'type:description', 'type:evaluation']"
28,rayyan-354359304,Exploring the design space of automatically synthesized hints for introductory programming assignments,"Suzuki R., Soares G., Glassman E., Head A., D'Antoni L., Hartmann B.","For massive programming classrooms, recent advances in program synthesis offer means to automatically grade and debug student submissions, and generate feedback at scale. A key challenge for synthesis-based autograders is how to design personalized feedback for students that is as effective as manual feedback given by teachers today. To understand the state of hint-giving practice, we analyzed 132 online Q&A posts and conducted a semi-structured interview with a teacher from a local massive programming class. We identified five types of teacher hints that can also be generated by program synthesis. These hints describe transformations, locations, data, behavior, and examples. We describe our implementation of three of these hint types. This work paves the way for future deployments of automatic, pedagogically-useful programming hints driven by program synthesis. Copyright © 2017 by the Association for Computing Machinery, Inc. (ACM).",2017,"['approach:unclear', 'data:internal_assignments', 'data_available:False', 'evaluation:teacher_survey', 'feedback:hints', 'feedback:program_repair', 'interaction:unknown', 'language:unknown_language', 'skill:correctness', 'technique:code_repair_for_feedback', 'technique:program_repair', 'tool:none', 'type:evaluation']"
29,rayyan-354359305,Developing applications to automatically grade introductory visual basic courses,Gerdes J.,"There are many unique challenges associated with introductory programming courses. For novice programmers, the challenges of their first programming class can lead to a great deal of stress and frustration. Regular programming assignments is often key to developing an understanding of best practices and the coding process. Students need practice with these new concepts to reinforce the underlying principles. Providing timely and consistent feedback on these assignments can be a challenge for instructors, particularly in large classes. Plagiarism is also a concern. Unfortunately traditional tools are not well suited to introductory courses. This paper describes how AppGrader, a static code assessment tool can be used to address the challenges of an introductory programming class. The tool assesses student’s understanding and application of programming fundaments as defined in the current ACM/IEEE Information Technology Curriculum Guidelines. Results from a bench test and directions for future research are provided. © 2017 AIS/ICIS Administrative Office. All Rights Reserved.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:plagiarism', 'interaction:single', 'language:vb', 'skill:correctness', 'skill:documentation', 'technique:dsl_rules', 'technique:language_stuctures', 'technique:static_analysis', 'technique:style_check', 'tool:Prutor', 'tool:appgrader', 'type:description', 'type:evaluation']"
30,rayyan-354359306,The impact of automated code quality feedback in programming education,"Jansen J., Oprescu A., Bruntink M.","While some university-level programming courses focus on software quality, often in introductory courses code quality is little touched upon due to time constraints. Students usually get feedback on code quality after the grading of their assignment, feedback that cannot be used on that same assignment. Our aim is to improve students' skills for code quality during the evolution of a students' programming assignment, while keeping the overhead low for teaching sta as well as for students. Better Code Hub is a service that checks code quality according to ten guidelines. We employ Better Code Hub as a formative assessment and feedback tool enabling students to monitor their progress on code quality. Our findings indicate that there is an improvement in the code quality of the students' assignments over the period the tool is used. Our experiments show that students benefited the most from feedback on unit length, unit complexity, and code duplication. Copyright c by the paper's authors. Copying permitted for private and academic purposes.",2017,"['approach:unclear', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'evaluation:feedback', 'feedback:static_analysis', 'interaction:unknown', 'language:python', 'skill:code_quality', 'technique:static_analysis', 'tool:better_code_hub', 'type:evaluation']"
31,rayyan-354359307,AuDoscore: Automatic grading of Java or scala homework,"Oster N., Kamp M., Philippsen M.","Fully automated test-based grading is crucial to cope with large numbers of student homework code. AuDoscore extends JUnit and keeps the task of creating exercises and corresponding grading tests simple. Students have a set of public smoke tests available. Grading also uses additional secret tests that check the submission more intensely. AuDoscore ensures that submissions cannot call libraries if the lecturer explicitly forbids them. Grading is not susceptible to the problem of consecutive faults by partially replacing student code with cleanroom code provided by a lecturer. AuDoscore can be run as a stand-alone application or integrated into our Exercise Submission Tool (EST). This paper briefly describes how both tools interact, depicts AuDoscore from the point of view of the lecturer, and describes some key technical aspects of its implementation.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:test_output', 'interaction:multiple', 'interaction:single', 'language:java', 'language:scala', 'skill:correctness', 'technique:test_cases', 'technique:unit_test_expansion', 'technique:unit_testing', 'tool:audoscore', 'type:description', 'type:evaluation']"
32,rayyan-354359308,Some aspects of grading java code submissions in MOOCs,"Király S., Nehéz K., Hornyák O.","Recently, massive open online courses (MOOCs) have been offering a new online approach in the field of distance learning and online education. A typical MOOC course consists of video lectures, reading material and easily accessible tests for students. For a computer programming course, it is important to provide interactive, dynamic, online coding exercises and more complex programming assignments for learners. It is expedient for the students to receive prompt feedback on their coding submissions. Although MOOC automated programme evaluation subsystem is capable of assessing source programme files that are in learning management systems, in MOOC systems there is a grader that is responsible for evaluating students’ assignments with the result that course staff would be required to assess thousands of programmes submitted by the participants of the course without the benefit of an automatic grader. This paper presents a new concept for grading programming submissions of students and improved techniques based on the Java unit testing framework that enables automatic grading of code chunks. Some examples are also given such as the creation of unique exercises by dynamically generating the parameters of the assignment in a MOOC programming course combined with the kind of coding style recognition to teach coding standards. © 2017, Taylor and Francis Ltd. All rights reserved.",2017,"['approach:fully_automated', 'data:internal_exams', 'data_available:False', 'evaluation:usage', 'feedback:test_output', 'interaction:unknown', 'language:java', 'skill:code_quality', 'skill:correctness', 'technique:dsl_rules', 'technique:style_check', 'technique:test_cases', 'technique:unit_testing', 'tool:checkstyle', 'tool:memooc', 'type:description']"
33,rayyan-354359309,An empirical study comparing two automatic graders for programming. MOOCs context,"Bey A., Jermann P., Dillenbourg P.","The present paper compares Algo+, an assessment tool for computer programs, to an automatic grader used in a MOOC course at EPFL. This empirical study explores the practicability and the behaviour of Algo+ and analyses whether Algo+ can be used to evaluate a large scale of programs. Algo+ is a prototype based on a static analysis approach for automated assessment of algorithms where programs are not executed but analysed by looking at their instructions. The second tool, EPFL grader, is used to grade programs submitted by students in MOOCs of Introductory programming with C++ at EPFL and is based on a compiler approach. In this technique submissions are assessed via a battery of unit tests where the student program is run with standard input and assessed on whether they produced the correct output. © Springer International Publishing AG 2017.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_graders', 'feedback:test_output', 'interaction:unknown', 'language:c++', 'skill:correctness', 'technique:pattern_matching', 'technique:test_cases', 'tool:epfl_grader', 'type:evaluation_paper']"
34,rayyan-354359310,Automatic assessment of programming assignments using image recognition,"Muuli E., Papli K., Tõnisson E., Lepp M., Palts T., Suviste R., Säde M., Luik P.","Automatic assessment of programming tasks in MOOCs (Massive Open Online Courses) is essential due to the large number of submissions. However, this often limits the scope of the assignments since task requirements must be strict for the solutions to be automatically gradable, reducing the opportunity for solutions to be creative. In order to alleviate this problem, we introduce a system capable of assessing the graphical output of a solution program using image recognition. This idea is applied to introductory computer graphics programming tasks whose solutions are programs that produce images of a given object on the screen. The image produced by the solution program is analysed using image recognition, resulting in a probability of a given object appearing in the image. The solution is accepted or rejected based on this score. The system was tested in a MOOC on 2,272 solution submissions. The results contained 4.6% cases of false negative and 0.5% cases of false positive grades. The method introduced in this paper saved approximately one minute per submission of the instructors’ time compared to manual grading. A participant survey revealed that the system was perceived to be functioning well or very well by 82.1% of the respondents, with an average rating of 4.4 out of 5. © Springer International Publishing AG 2017.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:user_study', 'feedback:probability_incorrect', 'interaction:single', 'language:python', 'skill:correctness', 'technique:graphical_output', 'technique:output_matching', 'tool:vpl', 'type:description', 'type:evaluation']"
35,rayyan-354359311,Experiences With Auto-Grading in a Systems Course,D. M. Rao,"Rapidly growing computing-class sizes across college campuses are challenging instructional resources and staffing. We have developed an automatic testing and grading software system called Code Assessment Extension (CODE). It is integrated with our existing Canvas Learning Management System (LMS). This paper presents experiences from both student and instructor perspectives with using auto-grading in a junior-level, systems course with a heavy emphasis on programming in C++, with several challenges, including - 1 this course was the first experience for both the students and the instructor in using any form of automatic grading, 2 the students have limited experience with C++ programming, particularly in Linux, and 3 the course includes complex concepts on operating systems, multithreading, and networking. The paper presents quantitative results from 3,300 submissions (from 1 course, 1 semester, 6 programming assignments, 54 students, multiple submissions per-student per-assignment) and analysis of end-of-course surveys from 54 students. The inferences from the statistics strongly support the use of automatic grading systems such as CODE to enhance learning in programming-centric courses.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'evaluation:auto_grading', 'evaluation:student_survey', 'feedback:test_output', 'interaction:multiple', 'language:c++', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:canvas', 'tool:code', 'type:description', 'type:evaluation']"
36,rayyan-354359313,Enhancing the Learning of Database Access Programming using Continuous Integration and Aspect Oriented Programming,B. Pérez,"Database access programming is a noteworthy component of Software Engineering (SE) education on databases that students are expected to acquire during training for their careers. In our university, we cover such an education in a course that emphasizes the use of the JDBC API to access databases. This paper presents our experiences in developing and running a framework to enhance the learning experience of database access programming, which is motivated by several factors. First, our students face great demands on acquiring JDBC acknowledge, and providing them with constructive feedback serves a critical role. Second, the increasing number of students leads to high efforts in managing and grading their assignments. Finally, we consider of strategic importance to bring modern industrial SE techniques into the classroom, so that students obtain a better experience with industry practices. Our framework draws upon constructive alignment and automated formative assessment, combining Continuous Integration (CI) and Aspect Oriented Programming (AOP). We include an innovative application of AOP, a programming technique that aims to modularize inherently scattered functionality into single functional units, to help students adopt well-established JDBC best practices. We also use well-known industrial software tools (Travis CI and GitHub) to manage and grade students' assignments and support automated integration testing with databases. The findings of this study, applied to a class of 53 students, suggest positive effects, such as motivate students to implement JDBC best practices, streamline the management and grading of their assignments, help them get familiar with industrial tools, or improve their grades.",2021,"['approach:semi_automatic', 'data:internal_exams', 'data_available:False', 'evaluation:student_survey', 'feedback:manual_personalised_feedback', 'feedback:test_output', 'interaction:multiple', 'language:java', 'skill:best_practices', 'skill:correctness', 'technique:ci_cd', 'technique:test_cases', 'technique:trace_examination', 'technique:unit_testing', 'tool:jdbc', 'type:description', 'type:evaluation']"
37,rayyan-354359314,MAESTRO: a semi-autoMAted Evaluation SysTem for pROgramming assignments,A. Bertagnon and M. Gavanelli,"Many works in the literature highlight the importance of formative assessment to improve the learning process. Formative assessment means providing assignments to the students and giving them feedback during the course. It is not very used in Italian Universities because it is highly time-consuming for the professors, that typically do not have help in the process of homework grading.In this work, we focus on programming exercises in computer science subjects, and propose a tool to semi-automatically grade and give feedback to the students. The tool was used in a computer language course on functional programming in a M.Sc. degree; the students evaluation of the course show a steep increase in the students appreciation. The tool is currently used in a course at undergraduate level on C programming.",2020,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_grading', 'evaluation:student_survey', 'feedback:manual_personalised_feedback', 'interaction:single', 'language:C', 'language:Haskell', 'skill:correctness', 'technique:property_based_testing', 'technique:test_cases', 'tool:flubaroo', 'tool:google_forms', 'tool:quickcheck', 'type:description', 'type:evaluation']"
38,rayyan-354359315,Quantitative Evaluation of Student Engagement in a Large-Scale Introduction to Programming Course using a Cloud-based Automatic Grading System,N. Norouzi and R. Hausen,"In this WIP Research to Practice paper, we explored the impact of integrating the university's learning management system and an automatic grading system in delivering a large-scale introduction to programming course in 2 consecutive quarters. Our initial approach utilizes an on-demand standalone automatic grading system and a separate assignment submission portal on Canvas. After evaluating our performance and specific student feedback, we integrated the assignment submission portal with the autograder system to provide a real-time objective assessment of assignments.The main improvement after enforcing assignment submission through the autograder (Stepik) was the noticeable improvement in the class average of assignment scores by 20.5% even though most of the test cases were hidden. Another interesting observation was the effect of our approach in decreasing the DFW rate to 12.5% from 46% and a considerable increase in the passing rate of female students, by 22%. We also noticed that in the second iteration of the course students who took the course as an elective were able to perform comparably and even better than students who took it as a requirement. It is also worth mentioning that using autograder helped students increase their code quality.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'evaluation:auto_grading', 'feedback:test_output', 'interaction:unknown', 'language:java', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:stepik', 'type:evaluation_paper']"
39,rayyan-354359316,The Influence of Test Suite Properties on Automated Grading of Programming Exercises,B. S. Clegg and P. McMinn and G. Fraser,"Automated grading allows for the scalable assessment of large programming courses, often using test cases to determine the correctness of students' programs. However, test suites can vary in multiple ways, such as quality, size, and coverage. In this paper, we investigate how much test suites with varying properties can impact generated grades, and how these properties cause this impact. We conduct a study on artificial faulty programs that simulate students' programming mistakes and test suites generated from manually written tests. We find that these test suites generate greatly varying grades, with the standard deviation of grades for each fault typically representing ~84% of the grades not apportioned to the fault. We show that different properties of test suites can influence the grades that they produce, with coverage typically making the greatest effect, and mutation score and the potentially redundant repeated coverage of lines also having a significant impact. We offer suggestions based on our findings to assist tutors with building grading test suites that assess students' code in a fair and consistent manner. These suggestions include ensuring that test suites have 100% coverage, avoiding unnecessarily recovering lines, and checking test suites using real or artificial faults.",2020,"['approach:unclear', 'data:code_defenders_dataset', 'data_available:False', 'evaluation:grading_accuracy', 'feedback:none', 'interaction:none', 'language:java', 'skill:correctness', 'technique:mutants', 'technique:test_cases', 'technique:unit_testing', 'tool:none', 'type:evaluation_paper']"
40,rayyan-354359317,Automated code evaluation of computer programming sessions with MATLAB Grader,Y. Boada and A. Vignoni,"Automatic grading of code and computational practices offers several beneficial practical and pedagogical advantages and is now possible using a variety of techniques. Recently, MathWorks has introduced a native solution for MATLAB code: MATLAB Grader. This web-based interface facilitates task submission and automatic grading based on preprogrammed criteria. The subject of Mechatronics (DISA-ETSINF-UPV) was used as a test where the in-class grading of some of the computer programming labs sessions have been replaced by automatically graded assignments, mainly using the MATLAB Grader platform.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:anecdotal', 'feedback:test_output', 'interaction:unknown', 'language:matlab', 'skill:correctness', 'technique:dsl_rules', 'technique:test_cases', 'technique:unit_testing', 'tool:matlab_grader', 'type:experience_report']"
41,rayyan-354359318,Gradeer: An Open-Source Modular Hybrid Grader,B. Clegg and M. -C. Villa-Uriol and P. McMinn and G. Fraser,"Automated assessment has been shown to greatly simplify the process of assessing students' programs. However, manual assessment still offers benefits to both students and tutors. We introduce Gradeer, a hybrid assessment tool, which allows tutors to leverage the advantages of both automated and manual assessment. The tool features a modular design, allowing new grading functionality to be added. Gradeer directly assists manual grading, by automatically loading code inspectors, running students' programs, and allowing grading to be stopped and resumed in place at a later time. We used Gradeer to assess an end of year assignment for an introductory Java programming course, and found that its hybrid approach offers several benefits.",2021,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:anecdotal', 'evaluation:usage', 'feedback:manual_personalised_feedback', 'interaction:single', 'language:java', 'skill:code_quality', 'skill:correctness', 'skill:readability', 'technique:style_check', 'technique:test_cases', 'technique:unit_testing', 'tool:checkstyle', 'tool:pmd', 'type:description', 'type:evaluation']"
42,rayyan-354359319,"2TSW: Automated Assessment of Computer Programming Assignments, in a Gamified Web Based System",G. Polito and M. Temperini and A. Sterbini,"Automated assessment and feedback of computer programming tasks can be a significant asset in computer science education. Web based systems providing such capabilities are designed to apply techniques ranging from static analysis of program correctness, to testing-based evaluation, and often can have application in frameworks supporting competitive programming. Here we report on the 2TSW system, which uses the testing-based approach in a gamified web-based environment. In 2TSWa learner can access a list of assignments, submit solutions and have such solutions tested and graded. The quality of the solutions contributes to the contents of the student's profile, showing experience points, medals gained for assignments, assignment categories badges, and an appropriate overall status badge. The personal profile allows the student to monitor her/his proceedings. The gamified structure of the system, together with the provision of real-time automated assessment, offers the opportunity for an increasing level of students' personal engagement and motivation. We describe the system, and provide the reader with the results of a field experimentation, conducted in a first-year computer programming course, bachelor in Computer Engineering. The data analysis of a questionnaire allows to conclude that the system was very welcome, and that the students appreciated the general gamified experience and the usefulness of the system. The data also allow to conclude that the students' engagement was high and that the learners appear to be open to the possibility of using again 2TSW and any other web-based system supporting assessment of complex tasks, also on subject matters different than Computer Programming.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:gamified', 'feedback:test_output', 'interaction:unknown', 'language:C', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:2tsw', 'type:description', 'type:evaluation']"
43,rayyan-354359320,Semantic Approach for Increasing Test Case Coverage in Automated Grading of Programming Exercise,M. R. I. Bariansyah and S. A. Rukmono and R. S. Perdana,"The widely popular approach for automatic grading in computer science is to run black-box testing against the student’s implementation. This kind of autograder evaluate programs solely based on their outputs given a set of inputs. However, manually writing a set of test cases with high coverage is laborious and inefficient. Hence, we explore another alternative approach in building test cases, specifically white-box testing. In theory, by knowing the internal workings of implementation, we can evaluate all possible execution paths, producing better test cases coverage, ultimately producing a complete grading. In this paper, we present research on using semantic analysis to generate test cases to determine the correctness of a student’s implementation. Instead of writing test cases, the evaluator will write a reference code, a correct implementation based on the programming problem specification. We implement a system that records execution paths, detects path deviation, and checks path equivalence to analyze the semantic difference of the reference code and student’s implementation. The system is built utilizing a concolic execution method for exploration and an SMT solver to solve formulas. Our experiments reveal that it is possible to automatically generate test cases and grade programming assignments by analyzing the semantic difference between reference and student implementation. Compared with grading using a random test case generator, it is evident that the system can provide better test case coverage for automatic grading in many occurrences.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:grading_accuracy', 'feedback:none', 'interaction:none', 'language:python', 'skill:correctness', 'technique:model_solution_req', 'technique:novel', 'technique:test_case_generation', 'technique:test_cases', 'tool:pyassessment', 'type:description', 'type:evaluation']"
44,rayyan-354359321,Automating Testing of Visual Observed Concurrency,P. Dewan and A. Wortas and Z. Liu and S. George and B. Gu and H. Wang,"Existing techniques for automating the testing of sequential programming assignments are fundamentally at odds with concurrent programming as they are oblivious to the algorithm used to implement the assignments. We have developed a framework that addresses this limitation for those object-based concurrent assignments whose user-interface (a) is implemented using the observer pattern and (b) makes apparent whether concurrency requirements are met. It has two components. The first component reduces the number of steps a human grader needs to take to interact with and score the user-interfaces of the submitted programs. The second component completely automates assessment by observing the events sent by the student-implemented observable objects. Both components are used to score the final submission and log interaction. The second component is also used to provide feedback during assignment implementation. Our experience shows that the framework is used extensively by students, leads to more partial credit, reduces grading time, and gives statistics about incremental student progress.",2021,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'feedback:manual_personalised_feedback', 'feedback:test_output', 'interaction:single', 'language:java', 'skill:concurrency', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:dbms', 'type:description', 'type:evaluation']"
45,rayyan-354359322,Enhancing an automated system for assessment of student programs using the token pattern approach,Y. T. Yu and C. M. Tang and C. K. Poon,"It is now common to use automated systems for assessing students' computer programming exercises. Many existing systems determine the correctness of a program by matching its output strings with the ones pre-deflned by the instructor. As a result, even when a student's program would be accepted as correct if marked by a human assessor, it is easily rejected by existing automated assessment systems as incorrect due to minor non-conformance of the program output. This technical limitation of existing systems is frequently a source of student complaints and frustrating learning experience. Common patches to these systems by simple pre-processing before matching the output strings are not satisfactory solutions. Recently, a token pattern approach has been proposed as a better solution by comparing the output tokens instead of characters. In this paper, we report our work of enhancing an existing automated program assessment system in our university by integrating it with the token pattern approach. Our preliminary evaluation shows that the enhanced system does improve the present state in that (1) it achieves progress towards more flexible assessment in a way closer to what a human assessor would normally do, and (2) more programming exercises are now assessable by the enhanced system with much reduced effort.",2017,"['approach:fully_automated', 'data:text_book_exercises', 'data_available:False', 'evaluation:compared_to_human', 'feedback:unclear', 'interaction:unknown', 'language:C', 'language:c++', 'skill:correctness', 'technique:output_matching', 'technique:pattern_matching', 'technique:token_pattern_matching', 'tool:pass', 'type:description', 'type:evaluation']"
46,rayyan-354359323,Smart Exam Evaluator for Object-Oriented Programming Modules,M. L. Wickramasinghe and H. P. Wijethunga and S. R. Yapa and D. M. D. Vishwajith and U. S. S. Samaratunge Arachchillage and N. Amarasena,"Worldwide educators considered that, automate the evaluation of programming language-based exams is a more challenging task due to its complexity and the diversity of solutions implemented by students. This research investigates and provides insight into the applicability and development of a java based online exam evaluator as a solution to traditional onerous manual exam assessment methodology. The proposed system allows students to take online exams in Java for an implemented source code in a practical exam, automatically reporting the results to the administrator simultaneously. Accordingly, this research examines existing methods, identifies their limitations, and explores the significance of introducing a smart object-oriented program-based exam evaluator as a solution. This method minimizes all human errors and makes the system more efficient. An automated answer checker checks and marks are given as human-counterpart and generate a report with possible suggestions for improvement of the answer scripts and generate a classification report to predict the student's final exam marks. This software application uses a Knowledge base, Abstract Syntax tree (AST), ANTLR, Image processing, and Machine Learning (ML) as key technologies. The proposed system gains a higher accuracy of 95% as performed by a separate human-counterpart. These results show a high level of accuracy and automate marking is the major emphasis to save human evaluation effort and maximize productivity.",2020,"['approach:fully_automated', 'data:internal_exams', 'data_available:False', 'evaluation:manual_grading', 'feedback:suggested_fixes', 'interaction:single', 'language:java', 'skill:correctness', 'technique:dsl_rules', 'technique:model_solution_req', 'technique:pattern_matching', 'tool:antlr', 'type:description', 'type:evaluation']"
47,rayyan-354359325,Automated Personalized Feedback in Introductory Java Programming MOOCs,V. J. Marin and T. Pereira and S. Sridharan and C. R. Rivero,"Currently, there is a ""boom"" in introductory programming courses to help students develop their computational thinking skills. Providing timely, personalized feedback that makes students reflect about what and why they did correctly or incorrectly is critical in such courses. However, the limited number of instructors and the great volume of submissions instructors need to assess, especially in Massive Open Online Courses (MOOCs), prove this task a challenge. One solution is to hire graders or create peer discussions among students, however, feedback may be too general, incomplete or even incorrect. Automatic techniques focus on: a) Functional testing, in which feedback usually does not sufficiently guide novices, b) Software verification to find code bugs, which may confuse novices since these tools usually skip true errors or produce false errors, and c) Comparing using reference solutions, in which a large amount of reference solutions or pre-existing correct submissions are usually required. This paper presents a semantic-aware technique to provide personalized feedback that aims to mimic an instructor looking for code snippets in student submissions. These snippets are modeled as subgraph patterns with natural language feedback attached to them. Submissions are transformed into extended program dependence graphs combining control and data flows. We leverage subgraph matching techniques to compute the adequate personalized feedback. Also, constraints correlating patterns allow performing fine-grained assessments. We have evaluated our method on several introductory programming assignments and a large number of submissions. Our technique delivered personalized feedback in milliseconds using a small set of patterns, which makes it appealing in real-world settings.",2017,"['approach:fully_automated', 'data:edX', 'data:internal_assignments', 'data_available:on_request', 'evaluation:analytics', 'feedback:personalised_feedback', 'interaction:none', 'language:java', 'skill:correctness', 'technique:dsl_rules', 'technique:graph_analysis', 'technique:knowledge_base', 'technique:model_solution_closeness', 'technique:pattern_matching', 'tool:antlr', 'tool:guava', 'tool:jgrapht', 'type:description', 'type:evaluation']"
48,rayyan-354359326,Automated Assessment of Complex Programming Tasks Using SIETTE,R. Conejo and B. Barros and M. F. Bertoa,"This paper presents an innovative method to tackle the automatic evaluation of programming assignments with an approach based on well-founded assessment theories (Classical Test Theory (CTT) and Item Response Theory (IRT)) instead of heuristic assessment as in other systems. CTT and/or IRT are used to grade the results of different items of evidence obtained from students' results. The methodology consists of considering program proofs as items, calibrating them, and obtaining the score using CTT and/or IRT procedures. These procedures measure overall validity reliability as well as diagnose the quality of each proof (item). The evidence is obtained through program proofs. The SIETTE system collects and processes all data to calculate the student knowledge level. This innovative method for programming task evaluation makes it possible to deploy the whole artillery developed in this research field over the last few decades. To the best of our knowledge, this is a new and original contribution in the area of programming assessment.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'feedback:test_output', 'interaction:multiple', 'language:C', 'language:language__agnostic', 'language:lex', 'language:yacc', 'skill:code_quality', 'skill:correctness', 'technique:output_matching', 'technique:test_cases', 'technique:unit_testing', 'tool:siette', 'type:description', 'type:evaluation']"
49,rayyan-354359327,A Partial Grading Method using Pattern Matching for Programming Assignments,X. Liu and Y. Kim and J. Cheon and G. Woo,"The common approach of the automatic judging systems of programming assignments is compiling the submitted assignment codes to executable programs, then executing them based on pre-defined inputs and grading scores based on the outputs. The output will be evaluated as zero points if it only contains partially correct answers in most judging systems. We apply an approach based on the pattern matching that is able to evaluate the assignment code and give it a partial score if its output is partially correct. Our approach is developed based on Scala and it can be deployed on our previous automatic online programming evaluation system named neoESPA.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:grading_accuracy', 'feedback:none', 'interaction:unknown', 'language:scala', 'skill:correctness', 'technique:output_matching', 'technique:pattern_matching', 'tool:neoespa', 'type:description', 'type:evaluation']"
50,rayyan-354359328,Question Independent Automated Code Analysis and Grading using Bag of Words and Machine Learning,K. K. Rai and B. Gupta and P. Shokeen and P. Chakraborty,Our first-hand experience with the recruiting process of companies helped us realize that existing systems judge coding responses solely on the basis of test cases passed and often additional manual involvement is needed to determine the quality of code. We aim to build an open source code base to automatically grade coding responses. This work will help save the tedious effort on the part of Subject Matter Experts (SMEs) by proving useful for grading responses in MOOCs (Massive Open Online Courses) as well as coding rounds in the recruitment process. We have built our work on foundations introduced in extant research in the area. We have achieved better precision (89%) in addition to better Pearson Correlation Coefficient (0.981) as compared to existing methods using the bag of words technique to calculate the distance vector. Various machine learning models were explored to achieve optimal results. These results and observations provide a promise of successful application of this technique in other areas requiring automated grading.,2019,"['approach:unclear', 'data:bespoke_assignment', 'data:dataset_available', 'data_available:False', 'evaluation:auto_graders', 'evaluation:ml_models', 'feedback:none', 'interaction:unknown', 'language:C', 'skill:code_quality', 'skill:correctness', 'skill:maintainability', 'technique:machine_learning', 'technique:ml', 'technique:model_solution_closeness', 'technique:model_solution_req', 'technique:nlp', 'technique:pattern_matching', 'technique:static_analysis', 'tool:cppcheck', 'type:description', 'type:evaluation']"
51,rayyan-354359330,EvalSeer: An Intelligent Gamified System for Programming Assignments Assessment,R. Nabil and N. E. Mohamed and A. Mahdy and K. Nader and S. Essam and E. Eliwa,"Continuous evaluation of computer programs and providing informative assessments are crucial for computer programming students. However, swift and formative feedback can be challenging to achieve as it is usually a stressful and tedious task for professors merely through manual grading. There is an urgent need for a Learning management system (LMS) that offers instant and detailed feedback in a competitive environment for a better education experience. In this study, we introduce the EvalSeer learning management system. EvalSeer is an LMS equipped with an intelligent auto-grading engine to keep learners motivated and help them move forward. The code evaluation process covers various criteria that strengthen coding abilities and provides learners with the directions they need to improve. These criteria include coding style, code features, dynamic test cases, and successful compilation. EvalSeer uses Long short-term memory (LSTM) networks for code analysis to detect syntax errors and predict potential fixes. Also, the system shall explain suggested fixes backed up with related references. EvalSeer is an easy-to-use cloud-based system with a learner-first approach that can be applied both on-campus and in elearning systems. This work is timely with the dramatic education change, with a notable rise of e-learning due to the COVID-19 pandemic.",2021,"['approach:fully_automated', 'data:github_dataset', 'data_available:False', 'evaluation:none', 'feedback:gamified', 'interaction:unknown', 'language:c++', 'skill:code_quality', 'skill:correctness', 'technique:ml', 'technique:program_repair', 'technique:static_analysis', 'technique:test_cases', 'technique:unit_testing', 'tool:cpplint', 'type:description']"
52,rayyan-354359331,Nudging Students Toward Better Software Engineering Behaviors,C. Brown and C. Parnin,"Student experiences in large undergraduate Computer Science courses are increasingly impacted by automated systems. Bots, or agents of software automation, are useful for efficiently grading and generating feedback. Current efforts at automation in CS education focus on supporting instructional tasks, but do not address student struggles due to poor behaviors, such as procrastination. In this paper, we explore using bots to improve the software engineering behaviors of students using developer recommendation choice architectures, a framework incorporating behavioral science concepts in recommendations to improve the actions of programmers. We implemented this framework in class-bot, a novel system designed to nudge students to make better choices while working on programming assignments. This work presents a preliminary evaluation integrating this tool in an introductory programming course. Our results show that class-bot is beneficial for improving student development behaviors increasing code quality and productivity.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:hints', 'feedback:nudge_theory', 'feedback:static_analysis', 'interaction:multiple', 'language:unknown_language', 'skill:code_quality', 'skill:correctness', 'skill:maintainability', 'skill:productivity', 'technique:test_cases', 'tool:checkstyle', 'type:description', 'type:evaluation']"
53,rayyan-354359332,PAAA: An Implementation of Programming Assignments Automatic Assessing System,"Tianyi S,Yulin K,Yihong H,Yujuan Q","Now we are in the information age, and distance learning is becoming more and more popular. For subjects related to computer science, programming assignments can't be avoided. But it's hard for human to review massive answers. Therefore, on the basis of former study, we put forward a new method, which is on the level of source code comprehension. In this paper, we revealed a method of programming assignment automatic assessing. Our project, Programming Assignments Automatic Assessing (PAAA), imitates human reviewers' action, and take such strategy to look through the source code. Furthermore, with the help of general grammar analyze tool ANTLR, it is able to read source code written in different languages, and build a language-independent program directed graph. Through a lot of tests, PAAA can be up to a lot of grading jobs of programming assignments assessment. And the result shows that PAAA's result is very close to human's marking.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:manual_grading', 'feedback:unclear', 'interaction:single', 'language:language__agnostic', 'skill:correctness', 'technique:graph_analysis', 'technique:model_solution_closeness', 'technique:model_solution_req', 'technique:pattern_matching', 'tool:antlr', 'type:description', 'type:evaluation']"
54,rayyan-354359333,Turn up the Heat! Using Heat Maps to Visualize Suspicious Code to Help Students Successfully Complete Programming Problems Faster,"Edmison B,Edwards SH","Automated grading systems provide feedback to students in a variety of ways, but they typically focus on identifying incorrect program behaviors. Such systems provide notices of test case failures or runtime errors, but without debugging skills, students often become frustrated when they don't know where to start addressing these defects. Borrowing from work in software engineering research related to automated defect location, we leverage previous research describes using statistical fault localization (SFL) techniques to identify the probable locations of defects in student coding assignments. The goal is to use these SFL techniques to provide a scaffold for students, to direct their debugging efforts without giving too much guidance, and thus minimizing the learning associated with investigating the defects. After determining the ""suspiciousness"" for each line of code involved in the defect, we create a ""heat map"" visualization overlay onto their source code of the ""suspiciousness"" scores to visually guide a student's attention to parts of their code that are most likely to contain problems.This paper describes the results of an analysis comparing two semesters of CS 1114: Introduction to Software Design (CS1) students, one which had access to the heat map feedback (Fall 2017: n 170), and one that did not (Fall 2015: n 270). The results show that when the heat maps were available, students found it easier to make improvements on their code from submission to submission, as well as spending less time overall achieving the maximum score on the instructor testing assessments. In fact, while we were optimistic in the impact of the heat map feedback, the results we observed were unexpectedly strong. To address this concern, we conducted an additional study, using student submissions from our Spring 2019 CS 1 students (n 230). The results of the second analysis confirmed the results of the first.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:with_without', 'feedback:personalised_feedback', 'feedback:static_analysis', 'feedback:visual_feedback', 'interaction:unknown', 'language:java', 'skill:code_quality', 'skill:correctness', 'technique:test_cases', 'tool:web_cat', 'type:description', 'type:evaluation']"
55,rayyan-354359334,Automatic Grading and Feedback Using Program Repair for Introductory Programming Courses,"Parihar S,Dadachanji Z,Singh PK,Das R,Karkare A,Bhattacharya A","We present GradeIT, a system that combines the dual objectives of automated grading and program repairing for introductory programming courses (CS1). Syntax errors pose a significant challenge for testcase-based grading as it is difficult to differentiate between a submission that is almost correct and has some minor syntax errors and another submission that is completely off-the-mark. GradeIT also uses program repair to help in grading submissions that do not compile. This enables running testcases on submissions containing minor syntax errors, thereby awarding partial marks for these submissions (which, without repair, do not compile successfully and, hence, do not pass any testcase). Our experiments on 15613 submissions show that GradeIT results are comparable to manual grading by teaching assistants (TAs), and do not suffer from unintentional variability that happens when multiple TAs grade the same assignment. The repairs performed by GradeIT enabled successful compilation of 56% of the submissions having compilation errors, and resulted in an improvement in marks for 11% of these submissions.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:manual_grading', 'feedback:program_repair', 'interaction:unknown', 'language:C', 'skill:correctness', 'technique:program_repair', 'technique:test_cases', 'tool:Prutor', 'tool:gradeit', 'type:description', 'type:evaluation']"
56,rayyan-354359336,Scaling up Functional Programming Education: Under the Hood of the OCaml MOOC,"Canou B,Di Cosmo R,Henry G","This article describes the key innovations used in the massive open online course ``Introduction to Functional Programming using OCaml'' that has run since the fall semester of 2015. A fully in-browser development environment with an integrated grader provides an exceptional level of feedback to the learners. A functional library of grading combinators greatly simplifies the notoriously complex task of writing test suites for the exercises, and provides static type-safety guarantees on the tested user code. Even the error-prone manual process of importing the course content in the learning platform has been replaced by a functional program that describes the course and statically checks its contents. A detailed statistical analysis of the data collected during and after the course assesses the effectiveness of these innovations.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:engagement', 'feedback:unclear', 'interaction:multiple', 'language:ocaml', 'skill:correctness', 'technique:language_stuctures', 'technique:model_solution_req', 'technique:style_check', 'technique:test_cases', 'technique:unit_testing', 'tool:none', 'type:description', 'type:experience_report']"
57,rayyan-354359337,Evaluation of a Tool for Java Structural Specification Checking,"Dil A,Osunde J","Although a number of tools for evaluating Java code functionality and style exist, little work has been done in a distance learning context on automated marking of Java programs with respect to structural specifications. Such automated checks support human markers in assessing students' work and evaluating their own marking; online automated marking; students checking code before submitting it for marking; and question setters evaluating the completeness of questions set. This project developed and evaluated a prototype tool that performs an automated check of a Java program's correctness with respect to a structural specification. Questionnaires and interviews were used to gather feedback on the usefulness of the tool as a marking aid to humans, and on its potential usefulness to students for self-assessment when working on their assignments. Markers were asked to compare the usefulness of structural specification testing as compared to other kinds of support, including syntax error assistance, style checking and functionality testing. Initial results suggest that most markers using the structural specification checking tool found it to be useful, and some reported that it increased their accuracy in marking. Reasons for not using the tool included lack of time and the simplicity of the assignment it was trialled on. Some reservations were expressed about reliance on tools for assessment, both for markers and for students. The need for advice on incorporating tools in marking workflow is suggested.",2018,"['approach:semi_automatic', 'data:none', 'data_available:False', 'evaluation:teacher_survey', 'feedback:test_output', 'interaction:unknown', 'language:java', 'skill:code_quality', 'skill:correctness', 'technique:language_stuctures', 'technique:test_cases', 'tool:bluej', 'tool:coderunner', 'tool:vle', 'type:evaluation']"
58,rayyan-354359338,Facilitating Course Assessment with a Competitive Programming Platform,"Coore D,Fokum D","We present an initiative that introduced the use of a competitive programming platform as a mechanism for auto-grading assignments for an introductory course on algorithm design and analysis. The specific objective of the intervention was to increase the number of assessed programming exercises to an average of 1 per week. A traditionally large enrolment with only a few graduate assistants available meant that prior to the intervention, few assignments were given, and the duration that students waited for feedback was long. Fresh problems were developed for deployment on the platform, each one targeting the specific learning objectives of the week in which they were given. The assignments were given in the format of a contest, and students were permitted to submit multiple attempts without penalty. There was a public leaderboard that showed real-time standings, but a student's grades depended only on the number of test cases his submissions passed and not on its ranking on the leaderboard. Anecdotally, we observed an increased degree of engagement with the course content. However a statistical analysis shows that the impact of the intervention on student performance, relative to previous instances of the course, was mixed. We discuss these and other findings.",2019,"['approach:oj', 'data:hackerrank', 'data:internal_assignments', 'data:internal_exams', 'data_available:True', 'evaluation:engagement', 'feedback:gamified', 'interaction:multiple', 'language:unknown_language', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:hackerrank', 'type:evaluation']"
59,rayyan-354359339,"An Open-Source, API-Based Framework for Assessing the Correctness of Code in CS50","Sharp C,van Assema J,Yu B,Zidane K,Malan DJ","We present check50, an open-source, extensible tool for assessing the correctness of students' code that provides a simple, functional framework for writing checks as well as an easy-to-use API that abstracts away common tasks, among them compiling and running programs, providing their inputs, and checking their outputs. As a result, check50 has allowed us to provide students with immediate feedback on their progress as they complete an assignment while also facilitating automatic and consistent grading, allowing teaching staff to spend more time giving tailored, qualitative feedback. We have found, though, that since introducing check50 in 2012 in CS50 at Harvard, students have begun to perceive the course's programming assignments as more time-consuming and difficult than in years past. We speculate that the feedback that check50 provides prior to students' submission of each assignment has compelled students to spend more time debugging than they had in the past. At the same time, students' correctness scores are now higher than ever.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:test_output', 'interaction:multiple', 'language:C', 'language:language__agnostic', 'language:python', 'skill:code_quality', 'skill:correctness', 'technique:static_analysis', 'technique:test_cases', 'technique:unit_testing', 'tool:check50', 'type:description', 'type:evaluation']"
60,rayyan-354359340,Autograding Interactive Computer Graphics Applications,"Maicus E,Peveler M,Aikens A,Cutler B","We present a system for the automated testing and grading of computer graphics applications. Our system runs, provides input to, and captures image and video output from graphical programming assignments. Instructors use a simple set of commands to script automated keyboard and mouse interactions with student programs at fixed times during execution. The resultant output, including plaintext standard output and mid-execution screenshots and GIFs, are displayed to the student to aid in debugging and ensure compliance with assignment specifications. Student output is automatically evaluated by basic text and image difference operations, or via an instructor-written validation method.We evaluate the success, implementation, and robustness of our design through deployment of this work in our university's senior undergraduate/graduate computer graphics course. In this course, students implement a variety of graphical assignments using OpenGL in C++. We summarize student feedback about the system gathered from anonymous end-of-term course evaluations. We provide anecdotal and quantitative evidence that the system improves student experience and learning by clarifying instructor expectations, building student confidence, and improving the consistency and efficiency of manual grading.This research has been implemented as an extension to Submitty, an open source, language-agnostic course management platform which allows automated testing and automated grading of student programming assignments. Submitty supports all levels of courses, from introductory to advanced special topics, and includes features for manual grading by TAs, version control, team submission, discussion forums, and plagiarism detection.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:student_survey', 'feedback:test_output', 'interaction:multiple', 'language:c++', 'language:opengl', 'skill:correctness', 'skill:graphics', 'technique:gui_testing', 'technique:test_cases', 'technique:unit_testing', 'tool:submitty', 'type:description', 'type:evaluation']"
61,rayyan-354359341,Source-Code Similarity Measurement: Syntax Tree Fingerprinting for Automated Evaluation,"Verma A,Udhayanan P,Shankar RM,Kn N,Chakrabarti SK","A majority of the current automated evaluation tools focus on grading a program based only on functionally testing the outputs. This approach suffers both false positives (i.e. finding errors where there are not any) and false negatives (missing out on actual errors). In this paper, we present a novel system which emulates manual evaluation of programming assignments based on the structure and not the functional output of the program using structural similarity between the given program and a reference solution. We propose an evaluation rubric for scoring structural similarity with respect to a reference solution. We present an ML based approach to map the system predicted scores to the scores computed using the rubric. Empirical evaluation of the system is done on a corpus of Python programs extracted from the popular programming platform, HackerRank, in combination with programming assignments submitted by students undertaking an undergraduate Python programming course. The preliminary results have been encouraging with the errors reported being as low as 12 percent with a deviation of about 3 percent, showing that the automatically generated scores are in high correlation with the instructor assigned scores.",2021,"['approach:fully_automated', 'data:hackerrank', 'data_available:True', 'evaluation:compared_to_human', 'feedback:unclear', 'interaction:unknown', 'language:unknown_language', 'skill:correctness', 'technique:ml', 'technique:model_solution_closeness', 'technique:model_solution_req', 'technique:pattern_matching', 'tool:none', 'type:description', 'type:evaluation']"
62,rayyan-354359342,Autograding Distributed Algorithms in Networked Containers,"Maicus E,Peveler M,Patterson S,Cutler B","We present a container-based system to automatically run and evaluate networked applications that implement distributed algorithms. Our implementation of this design leverages lightweight, networked Docker containers to provide students with fast, accurate, and helpful feedback about the correctness of their submitted code. We provide a simple, easy-to-use interface for instructors to specify networks, deploy and run instances of student and instructor code, and to log and collect statistics concerning node connection types and message content. Instructors further have the ability to control network features such as message delay, drop, and reorder. Running student programs can be interfaced with via stream-controlled standard input or through additional containers running custom instructor software. Student program behavior can be automatically evaluated by analyzing console or file output and instructor-specified rules regarding network communications. Program behavior, including logs of all messages passed within the system, can optionally be displayed to the student to aid in development and debugging. We evaluate the utility of this design and implementation for managing the submission and robust and secure testing of programming projects in a large enrollment theory of distributed systems course. This research has been implemented as an extension to Submitty, an open source, language-agnostic course management platform with automated testing and automated grading of student programming assignments. Submitty supports all levels of courses, from introductory to advanced special topics, and includes features for manual grading by TAs, version control, team submission, discussion forums, and plagiarism detection.",2019,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:none', 'feedback:in_person', 'interaction:single', 'language:unknown_language', 'skill:correctness', 'skill:distrbuted', 'technique:test_cases', 'technique:unit_testing', 'tool:docker', 'tool:submitty', 'type:description']"
63,rayyan-354359344,Applying Gamification to Motivate Students to Write High-Quality Code in Programming Assignments,"Kasahara R,Sakamoto K,Washizaki H,Fukazawa Y","Background: Traditional programming education focuses on training students' ability to write correct code that meets the specifications in programming assignments. In addition to correctness, software engineering studies argue that code quality is important. Problem: Nurturing students' ability to write high-quality code in programming assignments is difficult due to two main reasons. (1) Considering code quality while grading is undesirable because there are no objective and fair measurement metrics. (2) Grading assignments from multiple viewpoints (correctness and quality) is difficult and time-consuming. Approach: We propose applying gamification with code metrics to measure code quality in programming assignments. Our approach can motivate students to write code with good metric scores independent of grading. We implemented our approach and conducted a control experiment in a programming course at a university. Result: Our approach did not interfere with students' submissions but improved metric scores significantly. Hence, our approach can engage students to write high-quality code.",2019,"['approach:fully_automated', 'approach:oj', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:gamified', 'interaction:multiple', 'language:C', 'skill:code_quality', 'skill:correctness', 'skill:readability', 'technique:code_metrics', 'technique:metrics', 'technique:static_analysis', 'tool:woj', 'type:description', 'type:evaluation']"
64,rayyan-354359345,Effects of Human vs. Automatic Feedback on Students' Understanding of AI Concepts and Programming Style,"Leite A,Blanco SA","The use of automatic grading tools has become nearly ubiquitous in large undergraduate programming courses, and recent work has focused on improving the quality of automatically generated feedback. However, there is a relative lack of data directly comparing student outcomes when receiving computer-generated feedback and human-written feedback. This paper addresses this gap by splitting one 90-student class into two feedback groups and analyzing differences in the two cohorts' performance. The class is an intro to AI with programming HW assignments. One group of students received detailed computer-generated feedback on their programming assignments describing which parts of the algorithms' logic was missing; the other group additionally received human-written feedback describing how their programs' syntax relates to issues with their logic, and qualitative (style) recommendations for improving their code. Results on quizzes and exam questions suggest that human feedback helps students obtain a better conceptual understanding, but analyses found no difference between the groups' ability to collaborate on the final project. The course grade distribution revealed that students who received human-written feedback performed better overall; this effect was the most pronounced in the middle two quartiles of each group. These results suggest that feedback about the syntax-logic relation may be a primary mechanism by which human feedback improves student outcomes.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:on_request', 'evaluation:compared_to_human', 'feedback:fuzz_testing', 'interaction:unknown', 'language:python', 'skill:code_quality', 'skill:correctness', 'technique:static_analysis', 'technique:test_cases', 'tool:none', 'type:evaluation']"
65,rayyan-354359346,Automated Clustering and Program Repair for Introductory Programming Assignments,"Gulwani S,Radiček I,Zuleger F","Providing feedback on programming assignments is a tedious task for the instructor, and even impossible in large Massive Open Online Courses with thousands of students. Previous research has suggested that program repair techniques can be used to generate feedback in programming education. In this paper, we present a novel fully automated program repair algorithm for introductory programming assignments. The key idea of the technique, which enables automation and scalability, is to use the existing correct student solutions to repair the incorrect attempts. We evaluate the approach in two experiments: (I) We evaluate the number, size and quality of the generated repairs on 4,293 incorrect student attempts from an existing MOOC. We find that our approach can repair 97% of student attempts, while 81% of those are small repairs of good quality. (II) We conduct a preliminary user study on performance and repair usefulness in an interactive teaching setting. We obtain promising initial results (the average usefulness grade 3.4 on a scale from 1 to 5), and conclude that our approach can be used in an interactive setting.",2018,"['approach:fully_automated', 'data:bespoke_assignment', 'data_available:False', 'evaluation:compared_to_other_tools', 'evaluation:student_survey', 'feedback:program_repair', 'interaction:unknown', 'language:C', 'language:python', 'skill:correctness', 'technique:cluster', 'technique:clustering', 'technique:code_repair_for_feedback', 'technique:model_solution_closeness', 'technique:program_repair', 'tool:clara', 'type:description', 'type:evaluation']"
66,rayyan-354359347,Automatic Grading of Programming Assignments: An Approach Based on Formal Semantics,"Liu X,Wang S,Wang P,Wu D","Programming assignment grading can be time-consuming and error-prone if done manually. Existing tools generate feedback with failing test cases. However, this method is inefficient and the results are incomplete. In this paper, we present AutoGrader, a tool that automatically determines the correctness of programming assignments and provides counterexamples given a single reference implementation of the problem. Instead of counting the passed tests, our tool searches for semantically different execution paths between a student's submission and the reference implementation. If such a difference is found, the submission is deemed incorrect; otherwise, it is judged to be a correct solution. We use weakest preconditions and symbolic execution to capture the semantics of execution paths and detect potential path differences. AutoGrader is the first automated grading tool that relies on program semantics and generates feedback with counterexamples based on path deviations. It also reduces human efforts in writing test cases and makes the grading more complete. We implement AutoGrader and test its effectiveness and performance with real-world programming problems and student submissions collected from an online programming site. Our experiment reveals that there are no false negatives using our proposed method and we detected 11 errors of online platform judges.",2019,"['approach:fully_automated', 'data:codechef', 'data_available:on_request', 'evaluation:auto_graders', 'feedback:test_output', 'interaction:unknown', 'language:python', 'skill:correctness', 'technique:model_solution_closeness', 'technique:model_solution_req', 'technique:pattern_matching', 'technique:trace_examination', 'tool:autograder', 'tool:pathgrind', 'tool:pin', 'type:description', 'type:evaluation']"
67,rayyan-354359348,Effectiveness of Real-Time Feedback and Instructive Hints in Graduate CS Courses via Automated Grading System,Lee HH,"In this paper, we present our findings on enhancing personalized learning with the help of our real-time automated grading system for a graduate-level CS course. In this course, students learned how to use widely-used open-source libraries, and worked on 15 programming assignments with the ultimate goal of building an end-to-end big data processing pipeline in the cloud.We implemented our automated system to evaluate the correctness and efficiency of students' Java projects and to dynamically provide appropriate hints. The instructor complemented this real-time feedback with detailed, personalized feedback through frequent code reviews. In addition, a live, anonymous scoreboard allowed students to access grading results as well as to see the progress of their peers. The scoreboard and instructive hint system motivated the students to make incremental changes and also to spend enough time on testing one's own code before submitting it.In a survey, 97% of the students agreed that our grading system and personalized feedback enhanced their learning experiences. Given the overwhelmingly positive feedback from students, we believe that our findings could be helpful for those who are using or consider adopting auto-grading systems with the ultimate goal of providing enhanced learning experiences.",2021,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:gamified', 'feedback:manual_personalised_feedback', 'feedback:unit_testing', 'interaction:multiple', 'language:java', 'skill:correctness', 'technique:ci', 'technique:unit_testing', 'tool:jenkins', 'type:description', 'type:evaluation']"
68,rayyan-354359349,Automatic Assessment of OpenGL Computer Graphics Assignments,"Wünsche BC,Chen Z,Shaw L,Suselo T,Leung KC,Dimalen D,Mark W,Luxton-Reilly A,Lobb R","Teaching and learning computer graphics is often considered challenging due to it requiring a diverse range of skills such as mathematics, programming, problem solving, and art and design. Assignments are a popular tool to support learning and to assess students' understanding. The value of such assignments depends on the ability to give fast (and ideally formative) feedback, and enabling students to interactively explore the solution space. This is often a problem, in particular for large classes, where assignment marking can take many days or even weeks. By the time feedback is received students often don't remember details, and there is usually no opportunity to resubmit and hence little motivation to reflect on and correct mistakes. Previous work on assessing Computer Graphics assignments is rare and restricted to evaluating the quality of 3D models produced by students - usually using some form of image or mesh comparison, which only considers the final result, but not how it was obtained. In this paper we describe how to adapt CodeRunner, a free open-source question-type plug-in for Moodle, to OpenGL assignments, and our experience of using it with a class of about 300 students. Results were overwhelmingly positive and students perceived the tool as having significantly improved their learning.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:unclear', 'interaction:unknown', 'language:opengl', 'skill:correctness', 'technique:model_solution_req', 'technique:pattern_matching', 'technique:trace_examination', 'tool:coderunner', 'type:description', 'type:evaluation']"
69,rayyan-354359350,Auto-Grading Jupyter Notebooks,"Manzoor H,Naik A,Shaffer CA,North C,Edwards SH","Jupyter Notebooks are becoming more widely used, both for data science applications and as a convenient environment for learning Python. Currently, grading of assignments done in Jupyter Notebooks is typically done manually. Manual grading results in students receiving feedback only long after the assignment is complete. We implemented support for auto-grading programs written in Jupyter Notebooks within the Web-CAT auto-grading system. Scores received are directly reported to the Canvas gradebook. A Jupyter notebook extension allows students to upload their notebook files to Web-CAT directly. Survey results from class use show that 80% of students believe that getting immediate feedback from Web-CAT improved their performance. Instructors report that this implementation has significantly reduced their workload.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'evaluation:teacher_survey', 'feedback:test_output', 'interaction:unknown', 'language:python', 'skill:correctness', 'technique:model_solution_req', 'technique:test_cases', 'technique:unit_testing', 'tool:juypter', 'tool:web_cat', 'type:description', 'type:evaluation']"
70,rayyan-354359351,Automated Critique of Early Programming Antipatterns,"Ureel II LC,Wallace C","The introductory programming lab, with small cycles of teaching, coding, testing, and critique from instructors, is an extraordinarily productive learning experience for novice programmers. We wish to extend the availability of such critique through automation, capturing the essence of interaction between student and instructor as closely as possible. Integrated Development Environments and Automated Grading Systems provide constant feedback through static analysis and unit testing. But we also wish to tailor automated feedback to acknowledge commonly recurring issues with novice programmers, in keeping with the practice of a human instructor. We argue that the kinds of mistakes that novice programmers make, and the way they are reported to the novices, deserve special care. In this paper we provide examples of early programming antipatterns that have arisen from our teaching experience, and describe different ways of identifying and dealing with them automatically through our tool WebTA. Novice students may produce code that is close to a correct solution but contains syntactic errors; WebTA attempts to salvage the promising portions of the student's submission and suggest repairs that are more meaningful than typical compiler error messages. Alternatively, a student misunderstanding may result in well-formed code that passes unit tests yet contains clear design flaws; through additional analysis, WebTA can identify and flag them. Finally, certain types of antipattern can be anticipated and flagged by the instructor, based on the context of the course and the programming exercise; WebTA allows for customizable critique triggers and messages.",2019,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:dsl_rules', 'feedback:personalised_feedback', 'interaction:unknown', 'language:java', 'skill:code_quality', 'skill:maintainability', 'technique:antipatterns', 'technique:pattern_matching', 'tool:webta', 'type:description']"
71,rayyan-354359352,A Tutoring System to Learn Code Refactoring,"Keuning H,Heeren B,Jeuring J","In the last few decades, numerous tutoring systems and assessment tools have been developed to support students with learning programming, giving hints on correcting errors, showing which test cases do not succeed, and grading their overall solutions. The focus has been less on helping students write code with good style and quality. There are several professional tools that can help, but they are not targeted at novice programmers.This paper describes a tutoring system that lets students practice with improving small programs that are already functionally correct. The system is based on rules that are extracted from input by teachers collected in a preliminary study, a subset of rules taken from professional tools, and other literature. Rules define how a code construct can be rewritten into a better variant, without changing its functionality. Rules can be combined to form rewrite strategies, similar to refactorings offered by most IDEs. The student can ask for hints and feedback at each step.We describe the design of the system, show example sessions, and evaluate and discuss its contribution and limitations.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'evaluation:teacher_survey', 'feedback:dsl_rules', 'feedback:personalised_feedback', 'feedback:pre_defined_feedback', 'interaction:multiple', 'language:java', 'skill:code_quality', 'skill:refactoring', 'technique:pre_defined_questions', 'tool:refactortutor', 'type:description', 'type:evaluation']"
72,rayyan-354359353,Automating Systems Course Unit and Integration Testing: Experience Report,"Weikle DA,Lam MO,Kirkpatrick MS","Introducing software testing has taken on a greater importance in undergraduate computer science curricula in the last several years, with many departments using JUnit or other testing frameworks in the programming sequence and software engineering courses. We have developed an automated framework for unit and integration testing and grading for our intermediate-level systems course projects. Our system--designed to test C programs--combines the Check unit testing framework, custom Bash scripts for integration testing, and the Valgrind Memcheck memory leak detection tool. Although our courses use Linux, the framework is platform-independent and has been tested on a variety of other platforms. We have used this framework for seven semesters with four different instructors as part of the computer science program at a primarily undergraduate university with an emphasis on liberal arts. We distribute both public and private tests so that students get immediate feedback on their progress without knowing the actual contents of every test. We have observed that knowing their code is not completely working motivates more students to figure out what they don't understand before the project deadline. It also gives students examples of different levels of tests to use to debug their code, encourages them to develop a deeper understanding of the project specification, and reduces student anxiety about grades.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:anecdotal', 'feedback:test_output', 'interaction:multiple', 'language:C', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:check', 'type:experience_report']"
73,rayyan-354359354,Teaching the Art of Functional Programming Using Automated Grading (Experience Report),"Hameer A,Pientka B","Online programming platforms have immense potential to improve students' educational experience. They make programming more accessible, as no installation is required; and automatic grading facilities provide students with immediate feedback on their code, allowing them to to fix bugs and address errors in their understanding right away. However, these graders tend to focus heavily on the functional correctness of a solution, neglecting other aspects of students' code and thereby causing students to miss out on a significant amount of valuable feedback. In this paper, we recount our experience in using the Learn-OCaml online programming platform to teach functional programming in a second-year university course on programming languages and paradigms. Moreover, we explore how to leverage Learn-OCaml's automated grading infrastructure to make it easy to write more expressive graders that give students feedback on properties of their code beyond simple input/output correctness, in order to effectively teach elements of functional programming style. In particular, we describe our extensions to the Learn-OCaml platform that evaluate students on test quality and code style. By providing these tools and a suite of our own homework problems and associated graders, we aim to promote functional programming education, enhance students' educational experience, and make teaching and learning typed functional programming more accessible to instructors and students alike, in our community and beyond.",2019,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:test_output', 'interaction:multiple', 'language:ocaml', 'skill:code_design', 'skill:correctness', 'skill:test_suite_quality', 'technique:dsl_rules', 'technique:mutation_testing', 'technique:pre_defined_questions', 'technique:style_check', 'technique:test_cases', 'technique:unit_testing', 'tool:learn_ocaml', 'type:experience_report']"
74,rayyan-354359355,"In-Class Coding-Based Summative Assessments: Tools, Challenges, and Experience","Ju A,Mehne B,Halle A,Fox A","Pencil-and-paper coding questions on computer science exams are unrealistic: real developers work at a keyboard with extensive resources at hand, rather than on paper with few or no notes. We address the challenge of administering a proctored exam in which students must write code that passes instructor-provided test cases as well as writing test cases of their own. The exam environment allows students broad access to Internet resources they would use for take-home programming assignments, while blocking their ability to use that facility for direct communication with colluders. Our system supports cumulative questions (in which later parts depend on correctly answering earlier parts) by allowing the test-taker to reveal one or more hints by sacrificing partial credit for the question. Autograders built into the exam environment provide immediate feedback to the student on their exam grade. In case of grade disputes, a virtual machine image reflecting all of the student's work is preserved for later inspection. While elements of our scheme have appeared in the literature (autograding, semi-locked-down computer environments for exam-taking, hint ""purchasing""), we believe we are the first to combine them into a system that enables realistic in-class coding-based exams with broad Internet access. We report on lessons and experience creating and administering such an exam, including autograding-related pitfalls for high-stakes exams, and invite others to use and improve on our tools and methods.",2018,"['approach:fully_automated', 'data:internal_exams', 'data_available:False', 'evaluation:student_survey', 'evaluation:usage', 'feedback:unit_testing', 'interaction:single', 'language:ruby', 'skill:correctness', 'technique:property_based_testing', 'technique:test_cases', 'technique:unit_testing', 'tool:none', 'type:description', 'type:evaluation']"
75,rayyan-354359357,Characterizing the Pedagogical Benefits of Adaptive Feedback for Compilation Errors by Novice Programmers,U. Z. Ahmed and R. Sindhgatta and N. Srivastava and A. Karkare,"Can automated adaptive feedback for correcting erroneous programs help novice programmers learn to code better? In a large-scale experiment, we compare student performance when tutored by human tutors, and when receiving automated adaptive feedback. The automated feedback was designed using one of two well-known instructional principles: (i) presenting the correct solution for the immediate problem, or (ii) presenting generated examples or analogies that guide towards the correct solution. We report empirical results from a large-scale (N = 480,10,000+ person hour) experiment assessing the efficacy of these automated compilation-error feedback tools. Using the survival analysis on error rates of students measured over seven weeks, we found that automated feedback allows students to resolve errors in their code more efficiently than students receiving manual feedback. However, we also found that this advantage is primarily logistical and not conceptual; the performance benefit seen during lab assignments disappeared during exams wherein feedback of any kind was withdrawn. We further found that the performance advantage of automated feedback over human tutors increases with problem complexity, and that feedback via example and specific repair have distinct, non-overlapping relative advantages for different categories of programming errors. Our results offer a clear and granular delimitation of the pedagogical benefits of automated feedback in teaching programming to novices.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:True', 'evaluation:with_without', 'feedback:hints', 'feedback:program_repair', 'interaction:multiple', 'language:C', 'skill:correctness', 'technique:program_repair', 'tool:Prutor', 'tool:tracer', 'type:evaluation_paper']"
76,rayyan-354359359,Integrating the evaluation of out of the platform autoevaluated programming exercises with personalized answer in Open edX,I. Despujol and L. Salom and C. Turró,"This paper describes a procedure to integrate personalized self-evaluated programming exercises created in an external programming interactive environment using a standard problem type of a MOOC platform, making use of the anonymized identifier provided by the platform. We will explain how to integrate auto evaluated programming exercises with personalized answers created with Python notebooks, using the standard problem types that Open edX provides. We will review the alternatives we evaluated and why we discarded them and explain our final workflow with an example problem in the edx platform. In our workflow the autoevaluated programming exercises are created as if we were doing some test-driven development where the problem is our functionality and the unit tests are actually the verifications done to generate hints and evaluate the students. Once the problem is designed the unit tests create a code, based on the answer and the Anonymous userID, code that is obfuscated using an encryption technique. That code is used as the answer of a standard Open edX problem, creating a completely automated personalized environment and avoiding the use of Open Response Assessment tools that depend on the correction of other students.",2020,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:anecdotal', 'feedback:test_output', 'interaction:multiple', 'language:python', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:juypter', 'tool:mybinder', 'type:evaluation']"
77,rayyan-354359360,Promoting Code Quality via Automated Feedback on Student Submissions,O. Karnalim and Simon,"This research-to-practice work-in-progress paper presents an automated feedback tool that can be used in many teaching environments by integrating it with a web-based assessment submission system. Each time a student submits their work, they will automatically get feedback about aspects of the code quality. Automated feedback tools have been developed to educate students about code quality. However, integrating such a tool into an existing teaching environment can be challenging as these tools can depend on particular working environments, can be separate from the assessment submission system, or can require historical data. Our initial evaluation shows that the tool can be helpful as students do sometimes neglect to satisfy all code quality requirements. However, some false results are expected for spelling correction as student programs are not written in natural language. According to our quasi-experiments, the tool substantially reduces the number of word misspellings in comments due to their substantial frequency of occurrence.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'evaluation:with_without', 'feedback:hints', 'feedback:style_check', 'interaction:unknown', 'language:java', 'language:python', 'skill:code_quality', 'technique:static_analysis', 'tool:antlr', 'tool:ccs', 'type:description', 'type:evaluation']"
78,rayyan-354359361,TraceDiff: Debugging unexpected code behavior using trace divergences,R. Suzuki and G. Soares and A. Head and E. Glassman and R. Reis and M. Mongiovi and L. D'Antoni and B. Hartmann,"Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers' hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_other_tools', 'evaluation:user_study', 'feedback:hints', 'feedback:visual_feedback', 'interaction:multiple', 'language:python', 'skill:correctness', 'technique:dynamic_analysis', 'technique:model_solution_req', 'technique:pattern_matching', 'technique:program_repair', 'technique:trace_examination', 'tool:pythontutor', 'tool:tracediff', 'type:description', 'type:evaluation']"
79,rayyan-354359362,Measure Students’ Contribution in Web Programming Projects by Exploring Source Code Repository,B. -A. Nguyen and K. -Y. Ho and H. -M. Chen,"Large scale software systems are usually developed by multiple teams. Communication and collaboration skills are required as essential skills of qualified developers in the modern environment, in addition to the technical hard skills of software engineering. Therefore, group projects are included in almost courses to train students' professional skills and teamwork abilities. However, in their group projects, most of the students are only required to accomplish functional requirements with limited awareness about the quality of the software. This issue seriously affects the development speed, since it is difficult for team members to read others' code during the development process. Hence, we propose a quality-driven assessment system for group projects with the capability of checking the coding style after each submission and providing immediate feedback to help student teams maintain the code quality for their group projects. Besides, to objectively assess the contribution of all students in group projects, we propose a set of repository mining metrics including two major categories of productivity and quality. A case study about the usage of the system and the proposed contribution evaluation metrics are introduced to illustrate the feasibility of our approach.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:style_check', 'interaction:multiple', 'language:web_languages', 'skill:code_quality', 'technique:ci', 'tool:progedu4group', 'tool:sonarfix', 'type:description', 'type:evaluation']"
80,rayyan-354359363,Investigating the Essential of Meaningful Automated Formative Feedback for Programming Assignments,Q. Hao and J. P. Wilson and C. Ottaway and N. Iriumi and K. Arakawa and D. H. Smith,"This study investigated the essential of meaningful automated feedback for programming assignments. Three different types of feedback were tested, including (a) What's wrong- what test cases were testing and which failed, (b) Gap comparisons between expected and actual outputs, and (c) Hint hints on how to fix problems if test cases failed. 46 students taking a CS2 participated in this study. They were divided into three groups, and the feedback configurations for each group were different: (1) Group One -What's wrong, (2) Group Two -What's wrong + Gap, (3) Group Three -What's wrong+ Gap + Hint. This study found that simply knowing what failed did not help students sufficiently, and might stimulate system gaming behavior. Hints were not found to be impactful on student performance or their usage of automated feedback. Based on the findings, this study provides practical guidance on the design of automated feedback.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:with_without', 'feedback:personalised_feedback', 'interaction:multiple', 'language:java', 'skill:correctness', 'technique:ci', 'technique:test_cases', 'tool:travis', 'type:evaluation_paper']"
81,rayyan-354359364,Evaluating Feedback Tools in Introductory Programming Classes,R. Reis and G. Soares and M. Mongiovi and W. L. Andrade,"This Research Full Paper presents a study on the evaluation of feedback tools in introductory programming classes. Recently, several tools have been proposed in order to provide guidance and help students overcome conceptual difficulties in programming education. Some tools leverage clustering algorithms and program repair techniques to automatically generate personalized hints for students' incorrect programs. In contrast, some teachers choose to present students with program visualization tools to help them understand the dynamic execution of a source code. These tools are used to help students get correct solutions for programming assignments. However, due to limitations in assessments, it is still unclear how effective the feedback provided by these tools is. In this study, we analyzed the effectiveness of a tool for generating personalized hints and a tool for visualizing programs. To do so, we conducted a user study in which students, assisted by these tools, implemented solutions for three programming problems. Our results show that personalized hints can significantly reduce student's effort to get correct solutions. In addition, personalized hints can provide students with an understanding of problem solving similar to when using test cases. However, students who used the program visualization tool got lower post-test performance than using other tools.",2019,"['approach:fully_automated', 'data:bespoke_assignment', 'data_available:False', 'evaluation:user_study', 'evaluation:with_without', 'feedback:hints', 'feedback:visual_feedback', 'interaction:multiple', 'language:python', 'skill:correctness', 'technique:test_cases', 'tool:clara', 'tool:pythontutor', 'type:evaluation_paper']"
82,rayyan-354359366,End-to-End Automation of Feedback on Student Assembly Programs,Z. Liu and T. Liu and Q. Li and W. Luo and S. S. Lumetta,"We developed a set of tools designed to provide rapid feedback to students as they learn to write programs in assembly language (LC-3, a RISC-like educational instruction set architecture). At the heart of the system is an extended version of KLEE, KLC3, that enables us to both identify issues and perform equivalence checking between student code and a gold (correct) version of each assignment. Feedback begins when students edit their code using a VSCode extension that leverages static analysis to perform a variety of correctness and style checks, encouraging students to improve their code quality. Each time a student commits code to their Git repository, our system triggers. Using KLC3 (KLEE), the student code is executed along with the gold version, and issues and behavioral differences are delivered back to the student through their Git repository as a human-readable report, test cases, and scripts. A queueing system allows students to monitor progress, but responses are generally available within minutes. We also extended the LC-3 simulation tools to support reverse debugging, making the process of finding complex bugs much more tractable for students, and used Emscripten to develop a browser-based interface for use in testing and debugging. Finally, our system maintains an individual regression test suite for each student and requires a submission to pass all previous tests before re-evaluation in KLC3, thus avoiding encouraging programming-by-guesswork. We deployed the system to provide feedback for the assembly programming assignments in a class of over 100 students in Fall 2020. Students wrote a median of around 700 lines of assembly for these assignments, making heavy use of our tools to understand and eliminate their bugs. Anonymous student feedback on the tools was uniformly positive. Since that semester, we have continued to refine and expand our tools’ analysis capabilities and performance, and plan to deploy the system again in the near future (the class is offered every Fall).",2021,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:student_survey', 'evaluation:with_without', 'feedback:static_analysis', 'feedback:style_check', 'interaction:multiple', 'language:assesmbly', 'skill:correctness', 'technique:static_analysis', 'technique:test_case_generation', 'technique:test_cases', 'tool:LC_3', 'tool:klc3', 'tool:klee', 'type:description', 'type:evaluation']"
83,rayyan-354359367,Targeted Example Generation for Compilation Errors,U. Z. Ahmed and R. Sindhgatta and N. Srivastava and A. Karkare,"We present TEGCER, an automated feedback tool for novice programmers. TEGCER uses supervised classification to match compilation errors in new code submissions with relevant pre-existing errors, submitted by other students before. The dense neural network used to perform this classification task is trained on 15000+ error-repair code examples. The proposed model yields a test set classification Pred@3 accuracy of 97.7% across 212 error category labels. Using this model as its base, TEGCER presents students with the closest relevant examples of solutions for their specific error on demand. A large scale (N>230) usability study shows that students who use TEGCER are able to resolve errors more than 25% faster on average than students being assisted by human tutors.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:True', 'evaluation:compared_to_human', 'evaluation:with_without', 'feedback:program_repair', 'interaction:unknown', 'language:C', 'skill:correctness', 'technique:code_repair_for_feedback', 'technique:ml', 'technique:program_repair', 'tool:Prutor', 'tool:tegcer', 'type:description', 'type:evaluation']"
84,rayyan-354359368,How Automated Feedback is Delivered Matters: Formative Feedback and Knowledge Transfer,Q. Hao and M. Tsikerdekis,"This Full Research Paper investigated the correlations of automated formative feedback and student knowledge transfer in an advanced-level network course. A microservice of automated formative feedback was designed and implemented. The automated feedback was set up to address correctness and coding style along a series of milestones in finishing each of the assigned programming projects. 36 students participated in this study. Different from the concerns of prior studies that automated formative feedback may encourage trial & error and harm learning, this study identified a link between the automated formative feedback and student knowledge transfer. The results of this study confirmed positive findings from other fields on formative feedback, and call for more research on formative feedback in computing education.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data:internal_exams', 'data_available:False', 'evaluation:student_survey', 'feedback:style_check', 'feedback:test_output', 'interaction:multiple', 'language:unknown_language', 'skill:code_quality', 'skill:correctness', 'technique:static_analysis', 'technique:test_cases', 'tool:pylint', 'type:description', 'type:evaluation']"
85,rayyan-354359369,Annete: An Intelligent Tutoring Companion Embedded into the Eclipse IDE,M. Day and M. R. Penumala and J. Gonzalez-Sanchez,"With Computer Science (CS) class sizes that are often large, it is challenging to provide effective personalized feedback to students. Intelligent Tutoring Companions can provide such feedback and improve CS students' experience. This work describes the construction of a Tutoring Companion, Annete, designed to support students in a university Java programming course by providing them with intelligent feedback generated by a neural network. Annete is embedded into the Eclipse Integrated Development Environment (IDE), which is an environment that is already familiar to students in programming courses. Embedding Annete into Eclipse improves her effectiveness, as the students do not need to learn how to use an additional tool. While the student works in Eclipse, Annete collects 21 pieces of data from the student's code, including whether certain key words are used, error messages from the compiler, and cyclomatic complexity. When a run attempt, debug attempt, or a request for help occurs in Eclipse, Annete uses the data available to infer a feedback message to show to the student. Our approach is evaluated among 28 CS students completing a programming assignment while Annete assists them. Results suggest that students feel supported while working with Annete and show potential for using neural network modeling with embedded tutoring companions in the future. Challenges are discussed, as well as opportunities for future work.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:machine_learning', 'interaction:multiple', 'language:java', 'skill:code_quality', 'skill:correctness', 'technique:ml', 'tool:annete', 'type:description', 'type:evaluation']"
86,rayyan-354359370,Integration of virtual programming lab in a process of teaching programming EduScrum based,M. Cardoso and A. V. de Castro and A. Rocha,"Learning programming languages is essential for computer applications development and naturally for the technology evolution. The conventional teaching model of the subjects of initiation to programming is based on a set of tasks that usually result in a series of difficulties for the students which leads to an exhaustive monitoring by the teachers. It turns out that one of the main problems is that students often do not know if their work is correct (i.e. whether it works for a pre-defined data set) until it is reviewed and evaluated by the teacher. In this context, the Virtual Learning Environment becomes an attractive and interesting tool to help teach (and learn) programming. One of these tools is VPL (Virtual Programming Lab) a plugin of Moodle (Modular Object-Oriented Dynamic Learning Environment). We believe that the use of VPL may represent a paradigm shift for students, but also, and especially for teachers, because teachers will have to spend much more time upstream to prepare tasks and activities, test the environment and verify validation of the data. However, it is important to note that downstream teacher effort can significantly reduce because students do not need as much attention given that feedback on the quality and functionality of the code developed is immediate. This work is a part of a PhD in Research at Information Technology in the University of Santiago de Compostela, Spain, and has been developed from the last 12 months. This Ph.D. research aims to analyze the VPL's integration potential in the programming teaching-learning process in higher education. It also aims to verify if, with VPL, it is possible to make student's learning more effective and autonomous, with less time and effort spent by teachers in assignments evaluation. In the present article, we will present a work-in-progress where we will show how VPL was introduced in a teaching-learning process supported on JAVA programming. The teaching method uses EduScrum methodology and is included in the course of Computer Engineering of the Department of Informatics Engineering of the School of Engineering of the Polytechnic Institute of Porto (ISEP / P.PORTO). The first results of this process will be presented.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:test_output', 'interaction:single', 'language:java', 'skill:correctness', 'technique:test_cases', 'tool:vpl', 'type:experience_report']"
87,rayyan-354359372,Building a Comprehensive Automated Programming Assessment System,I. Mekterović and L. Brkić and B. Milašinović and M. Baranović,"Automated Programming Assessment Systems (APAS) are used for overcoming problems associated with manually managed programming assignments, such as objective and efficient assessing in large classes and providing timely and helpful feedback. In this paper we survey the literature and software in this field and identify the set of necessary features that make APAS comprehensive - such that it can support all key stages in the assessment process. Put differently, comprehensive APAS is generic enough to meet the demands of “any” computer science course. Despite the vast number of publications, the choice of software turns out to be very limited. We contribute by developing Edgar, a comprehensive open-source APAS which, to the best of our knowledge, exceeds any other similar free and/or open-source tool. Edgar is the result of three years of development and usage in, for the time being, eight courses dealing with various programming languages and paradigms (C, Java, SQL, etc.). Edgar supports various text-based programming languages, multi-correct multiple-choice questions, provides rich exam logging and monitoring infrastructure to prevent potential fraudulent behaviour, and subsequent data analysis and visualization of students' scores, exams, question quality, etc. It can be deployed on all major operating systems and is written in a modular fashion so that it can be adjusted and scaled to a custom fit. We comment on the architecture and present data from real-world use-cases to support these claims. Edgar is in active use today (1000+ students per semester) and it is being constantly developed with new features.",2020,"['approach:fully_automated', 'data:internal_exams', 'data_available:False', 'evaluation:compared_to_other_tools', 'feedback:test_output', 'interaction:single', 'language:language__agnostic', 'skill:correctness', 'technique:output_matching', 'technique:test_cases', 'technique:unit_testing', 'tool:none', 'type:description', 'type:evaluation']"
88,rayyan-354359373,AOCO - A Tool to Improve the Teaching of the ARM Assembly Language in Higher Education,J. Damas and B. Lima and A. J. Araújo,"Assessment is an important part of the educational process, playing a crucial role in student learning. The increase in the number of students in higher education has placed extreme pressure on assessment practices, often leading to a teacher having hundreds of assignments to correct, not only giving feedback too late, but also low quality feedback, as it is humanly impossible to correct all these assessments by giving quality feedback in such a short time. Due to the social confinement caused by the pandemic of COVID-19, there was the need to change the evaluation method initially associated with a thin exam, to a continuous evaluation method based on multiple weekly assignments. In order to deal with this situation, we developed AOCO, the first automatic correction tool for the ARMv8 AArch64 assembly language. This work presents the AOCO tool, as well as the results of the evaluation of a first use with students",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:test_output', 'interaction:unknown', 'language:assesmbly', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:aoco', 'type:description', 'type:evaluation']"
89,rayyan-354359375,Context-aware and data-driven feedback generation for programming assignments,"Song D., Lee W., Oh H.","Recently, various techniques have been proposed to automatically provide personalized feedback on programming exercises. The cutting edge of which is the data-driven approaches that leverage a corpus of existing correct programs and repair incorrect submissions by using similar reference programs in the corpus. However, current data-driven techniques work under the strong assumption that the corpus contains a solution program that is close enough to the incorrect submission. In this paper, we present Cafe, a new data-driven approach for feedback generation that overcomes this limitation. Unlike existing approaches, Cafe uses a novel context-aware repair algorithm that can generate feedback even if the incorrect program differs significantly from the reference solutions. We implemented Cafe for OCaml and evaluated it with 4,211 real student programs. The results show that Cafe is able to repair 83 % of incorrect submissions, far outperforming existing approaches. © 2021 ACM.",2021,"['approach:fully_automated', 'data:bespoke_assignment', 'data_available:True', 'evaluation:compared_to_other_tools', 'evaluation:student_survey', 'feedback:program_repair', 'interaction:unknown', 'language:ocaml', 'skill:correctness', 'technique:code_repair_for_feedback', 'technique:model_solution_req', 'technique:pattern_matching', 'technique:program_repair', 'tool:cafe', 'type:description', 'type:evaluation']"
90,rayyan-354359376,Using Virtual Programming Lab to improve learning programming: The case of Algorithms and Programming,"Cardoso M., Marques R., de Castro A.V., Rocha Á.","Programming is one of the basic skills that students must acquire. However, learning to program is not an easy task. Also teaching programming is an arduous but challenging task, requiring close follow-up and constant and meaningful feedback. So the main question is: how can we help teachers and students to achieve these goals? We identified a tool that can be useful to this purpose. That is Virtual Programming Lab (VPL), a Moodle plugin that allows students to submit their code and get prompt feedback without the teacher's intervention. In order to test this concept, an experiment was performed with several classes of beginner programming students, in two editions of Algorithms and Programming course unit of the degree in Informatics Engineering lectured at the Informatics Engineering Department at the School of Engineering, Polytechnic Institute of Porto. The students were challenged to test their assignments in VPL with a set of test values previously defined by the teachers. After the experiments, we used surveys to gather the involved students' and teachers' opinion, and more than 70% of the students answered that they considered the VPL an added value for the teaching–learning process. The dynamics verified in the classes, the general opinion of the teachers, and the acceptance and participation of the students allow to classify the experience as positive. © 2020 John Wiley & Sons, Ltd",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'evaluation:teacher_survey', 'feedback:style_check', 'feedback:test_output', 'interaction:single', 'language:java', 'skill:correctness', 'technique:test_cases', 'tool:vpl', 'type:experience_report']"
91,rayyan-354359377,Fast and accurate incremental feedback for students’ software tests using selective mutation analysis,"Kazerouni A.M., Davis J.C., Basak A., Shaffer C.A., Servant F., Edwards S.H.","As incorporating software testing into programming assignments becomes routine, educators have begun to assess not only the correctness of students’ software, but also the adequacy of their tests. In practice, educators rely on code coverage measures, though its shortcomings are widely known. Mutation analysis is a stronger measure of test adequacy, but it is too costly to be applied beyond the small programs developed in introductory programming courses. We demonstrate how to adapt mutation analysis to provide rapid automated feedback on software tests for complex projects in large programming courses. We study a dataset of 1389 student software projects ranging from trivial to complex. We begin by showing that although the state-of-the-art in mutation analysis is practical for providing rapid feedback on projects in introductory courses, it is prohibitively expensive for the more complex projects in subsequent courses. To reduce this cost, we use a statistical procedure to select a subset of mutation operators that maintains accuracy while minimizing cost. We show that with only 2 operators, costs can be reduced by a factor of 2–3 with negligible loss in accuracy. Finally, we evaluate our approach on open-source software and report that our findings may generalize beyond our educational context. © 2021 The Author(s)",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'feedback:test_output', 'interaction:unknown', 'language:java', 'skill:correctness', 'skill:test_suite_quality', 'technique:mutation_testing', 'technique:test_cases', 'tool:web_cat', 'type:description', 'type:evaluation']"
92,rayyan-354359378,A Comparison of Inquiry-Based Conceptual Feedback vs. Traditional Detailed Feedback Mechanisms in Software Testing Education: An Empirical Investigation,"Cordova L., Carver J., Gershmel N., Walia G.","The feedback provided by current testing education tools about the deficiencies in a student's test suite either mimics industry code coverage tools or lists specific instructor test cases that are missing from the student's test suite. While useful in some sense, these types of feedback are akin to revealing the solution to the problem, which can inadvertently encourage students to pursue a trial-and-error approach to testing, rather than using a more systematic approach that encourages learning. In addition to not teaching students why their test suite is inadequate, this type of feedback may motivate students to become dependent on the feedback rather than thinking for themselves. To address this deficiency, there is an opportunity to investigate alternative feedback mechanisms that include a positive reinforcement of testing concepts. We argue that using an inquiry-based learning approach is better than simply providing the answers. To facilitate this type of learning, we present Testing Tutor, a web-based assignment submission platform that supports different levels of testing pedagogy via a customizable feedback engine. We evaluated the impact of the different types of feedback through an empirical study in two sophomore-level courses. We use Testing Tutor to provide students with different types of feedback, either traditional detailed code coverage feedback or inquiry-based learning conceptual feedback, and compare the effects. The results show that students that receive conceptual feedback had higher code coverage (by different measures), fewer redundant test cases, and higher programming grades than the students who receive traditional code coverage feedback. © 2021 ACM.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:True', 'evaluation:student_survey', 'feedback:personalised_feedback', 'feedback:static_analysis', 'interaction:multiple', 'language:java', 'skill:test_suite_quality', 'technique:unclear', 'tool:testingtutor', 'type:evaluation_paper']"
93,rayyan-354359379,Promoting Early Engagement with Programming Assignments Using Scheduled Automated Feedback,"Denny P., Whalley J., Leinonen J.","Programming assignments are a common form of assessment in introductory courses and often require substantial work to complete. Students must therefore plan and manage their time carefully, especially leading up to published deadlines. Although time management is an important metacognitive skill that students must develop, it is rarely taught explicitly. Prior research has explored various approaches for reducing procrastination and other unproductive behaviours in students, but these are often ineffective or impractical in large courses. In this work, we investigate a scalable intervention that incentivizes students to begin work early. We provide automatically generated feedback to students who submit their work-in-progress prior to two fixed deadlines scheduled earlier than the final deadline for the assignment. Although voluntary, we find that many students welcome this early feedback and improve the quality of their work across each iteration. Especially for at-risk students, who have failed an earlier module in the course, engaging with the early feedback opportunities results in significantly better work at the time of final submission. © 2021 ACM.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:grade_correlation', 'feedback:personalised_feedback', 'feedback:unit_testing', 'interaction:multiple', 'language:C', 'language:matlab', 'skill:correctness', 'technique:test_cases', 'tool:none', 'type:evaluation']"
94,rayyan-354359381,A gamified web based system for computer programming learning,"Polito G., Temperini M.","The availability of Automated Assessment tools for computer programming tasks can be a significant asset in Computer Science education. Systems providing such kind of service are built around an interface, allowing to administer the tasks (exercises to train programming skills), and show the results, accompanied by meaningful feedback. To produce such results, they apply techniques ranging from static analysis of program correctness, to testing-based evaluation. These systems can also support Competitive Programming, which is known to have educational meaning too. We developed the 2TSW system, supporting the automated correction of computer programming tasks, in a gamified web-based environment. The system let the student access a list of assignments (programming tasks), submit solutions to them, and have such solutions tested and graded. Accomplished tasks let the student gain experience points, represented also by medals, recognition of mastery on a topic, and improvements on a personal characterization of the student's status. The personal profile allows the student to monitor her/his proceedings and achievements. The gamified structure of the system, together with the availability of real-time automated assessment, offers the opportunity for an increasing level of students' personal engagement and motivation. Here we describe the system, and report on an experimentation, where students of a Bachelor Programme in Computer Engineering, first year, used 2TSW. In particular, we 1) present findings about the students' feedback, coming from a questionnaire administered after the experience, and 2) provide the reader with an analysis of the participation data, based on simple statistic tests. The students' feedback let us conclude that they appreciated the 2TSW gamified experience, perceived the system as useful, and maintained a high level of engagement. The data analysis allowed for less decisive conclusions, although it showed proof of the effectiveness of the system as a learning aid. © 2021 The Authors",2021,"['approach:fully_automated', 'data:bespoke_assignment', 'data_available:False', 'evaluation:correlation_grades', 'evaluation:student_survey', 'feedback:gamified', 'interaction:multiple', 'language:C', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:2tsw', 'type:description', 'type:evaluation']"
95,rayyan-354359383,Automated Assessment of Learning Objectives in Programming Assignments,"Rump A., Fehnker A., Mader A.","Individual feedback is a core ingredient of a personalised learning path. However, it also is time-intensive and, as a teaching form, it is not easily scalable. In order to make individual feedback realisable for larger groups of students, we develop tool support for teaching assistants to use in the process of giving feedback. In this paper, we introduce Apollo, a tool that automatically analyses code uploaded by students with respect to their progression towards the learning objectives of the course. First, typical learning objectives in Computer Science courses are analysed on their suitability for automated assessment. A set of learning objectives is analysed further to get an understanding of what achievement of these objectives looks like in code. Finally, this is implemented in Apollo, a tool that assesses the achievement of learning objectives in Processing projects. Early results suggest an agreement in assessment between Apollo and teaching assistants. © 2021, Springer Nature Switzerland AG.",2021,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:on_request', 'evaluation:compared_to_human', 'feedback:personalised_feedback', 'interaction:unknown', 'language:java', 'skill:correctness', 'skill:maintainability', 'skill:readability', 'technique:code_metrics', 'technique:static_analysis', 'tool:apollo', 'tool:atelier', 'tool:pmd', 'type:description', 'type:evaluation']"
96,rayyan-354359385,Towards understanding the effective design of automated formative feedback for programming assignments,"Hao Q., Smith IV D.H., Ding L., Ko A., Ottaway C., Wilson J., Arakawa K.H., Turcan A., Poehlman T., Greer T.","Background and Context: automated feedback for programming assignments has great potential in promoting just-in-time learning, but there has been little work investigating the design of feedback in this context. Objective: to investigate the impacts of different designs of automated feedback on student learning at a fine-grained level, and how students interacted with and perceived the feedback. Method: a controlled quasi-experiment of 76 CS students, where students of each group received a different combination of three types of automated feedback for their programming assignments. Findings: feedback addressing the gap between expected and actual outputs is critical to effective learning; feedback lacking enough details may lead to system gaming behaviors. Implications: the design of feedback has substantial impacts on the efficacy of automated feedback for programming assignments; more research is needed to extend what is known about effective feedback design in this context. © 2021 Informa UK Limited, trading as Taylor & Francis Group.",2021,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_graders', 'evaluation:student_survey', 'feedback:personalised_feedback', 'feedback:unit_testing', 'interaction:multiple', 'language:java', 'skill:correctness', 'technique:ci', 'technique:test_cases', 'tool:travis', 'type:evaluation_paper']"
97,rayyan-354359386,Nudging student learning strategies using formative feedback in automatically graded assessments,"Zamprogno L., Holmes R., Baniassad E.","Automated assessment tools are widely used as a means for providing formative feedback to undergraduate students in computer science courses while helping those courses simultaneously scale to meet student demand. While formative feedback is a laudable goal, we have observed many students trying to debug their solutions into existence using only the feedback given, while losing context of the learning goals intended by the course staff. In this paper, we detail two case studies from second and third-year undergraduate software engineering courses indicating that using only nudges about where students should focus their efforts can improve how they act on generated feedback. By carefully reasoning about errors uncovered by our automated assessment approaches, we have been able to create feedback for students that helps them to revisit the learning outcomes for the assignment or course. This approach has been applied to both multiple-choice feedback in an online quiz taking system and automated assessment of student programming tasks. We have found that student performance has not suffered and that students reflect positively about how they investigate automated assessment failures. © 2020 ACM.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:nudge_theory', 'feedback:personalised_feedback', 'feedback:test_output', 'feedback:unit_testing', 'interaction:multiple', 'language:typescript', 'skill:code_quality', 'skill:correctness', 'technique:test_cases', 'tool:autotest', 'tool:checkstyle', 'type:description', 'type:evaluation']"
98,rayyan-354359387,Student Refactoring Behaviour in a Programming Tutor,"Keuning H., Heeren B., Jeuring J.","Producing high-quality code is essential for professionals working on maintainable software. However, awareness of code quality is also important for novices. In addition to writing programs meeting functional requirements, teachers would like to see their students write understandable, concise and efficient code. Unfortunately, time to address these qualitative aspects is limited. We have developed a tutoring system for programming that teaches students to refactor functionally correct code, focussing on the method-level. The tutoring system provides automated feedback and layered hints. This paper describes the results of a study of 133 students working with the tutoring system. We analyse log data to see how they approach the exercises, and how they use the hints and feedback to refactor code. In addition, we analyse the results of a student survey. We found that students with some background in programming were generally able to identify issue in code and solve them (on average 92%), that they used hints at various levels, and we noticed occasional learning in recurring issues. They struggled most with simplifying complex control flow. Students generally valued the topic of code quality and working with the tutor. Finally, we derive improvements for the tutoring system to strengthen students' comprehension of refactoring. © 2020 ACM.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'evaluation:student_survey', 'feedback:dsl_rules', 'feedback:personalised_feedback', 'feedback:unit_testing', 'interaction:multiple', 'language:c#', 'skill:code_quality', 'skill:correctness', 'skill:refactoring', 'technique:pre_defined_questions', 'technique:test_cases', 'tool:refactortutor', 'type:description', 'type:evaluation']"
99,rayyan-354359388,Customizable and scalable automated assessment of C/C++ programming assignments,"Delgado-Pérez P., Medina-Bulo I.","The correction of exercises in programming courses is a laborious task that has traditionally been performed in a manual way. This situation, in turn, delays the access by students to feedback that can contribute significantly to their training as future professionals. Over the years, several approaches have been proposed to automate the assessment of students' programs. Static analysis is a known technique that can partially simulate the process of manual code review performed by lecturers. As such, it is a plausible option to assess whether students' solutions meet the requirements imposed on the assignments. However, implementing a personalized analysis beyond the rules included in existing tools may be a complex task for the lecturer without a mechanism that guides the work. In this paper, we present a method to provide automated and specific feedback to immediately inform students about their mistakes in programming courses. To that end, we developed the CAC++ library, which enables constructing tailored static analysis programs for C/C++ practices. The library allows for great flexibility and personalization of verifications to adjust them to each particular task, overcoming the limitations of most of the existing assessment tools. Our approach to providing specific feedback has been evaluated for a period of three academic years in a course related to object-oriented programming. The library allowed lecturers to reduce the size of the static analysis programs developed for this course. During this period, the academic results improved and undergraduates positively valued the aid offered when undertaking the implementation of assignments. © 2020 Wiley Periodicals LLC",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:test_output', 'interaction:single', 'language:c++', 'skill:code_design', 'skill:correctness', 'technique:dsl_rules', 'technique:test_cases', 'tool:cac++', 'type:description', 'type:evaluation']"
100,rayyan-354359390,CLIK: Cloud-based Linux kernel practice environment and judgment system,"Park H., Kim Y.","Assignments on kernel programming are essential parts of operating system (OS) courses taught to computer science students to provide them a deep understanding of real-world OSs. However, these assignments require tremendous effort from both students and instructors. Students are routinely flustered by the daunting task of building a practice environment from scratch; instructors are pressurized for time while validating a student's work that requires several kernel installations and reboots. To minimize this effort, we propose CLIK, a cloud-based Linux kernel practice environment supporting automatic judgment. It provides students with an individual and easy-to-use kernel practice environment and instructors with a fast and easy evaluation of students' work with live feedback. Our experiences with two assignments from a real-world OS course carried out on CLIK show that CLIK can successfully provide Linux kernel environments for 40 students and help instructors greatly by validating their kernels within one minute using parallel and automated judgments. We also describe detailed lessons learned from developing CLIK that will help both researchers and instructors building similar systems. © 2020 The Authors. Computer Applications in Engineering Education published by Wiley Periodicals LLC",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_graders', 'feedback:unit_testing', 'interaction:unknown', 'language:kernal', 'skill:correctness', 'technique:test_cases', 'tool:clik', 'type:description', 'type:evaluation']"
101,rayyan-354359391,The Impact of Iterative Assessment System on Programming Learning Behavior,"Yan Y.-X., Wu J.-P., Nguyen B.-A., Chen H.-M.","Automated programming assessment systems (APAS) are useful supporting tools adopted in novice programming courses. They allow educators to reduce the amount of work required for homework assessment as well as students to have feedback and correct their code. In this study, we analyzed students' learning behavior from an APAS, called ProgEdu, which provides an iterative learning environment for object-oriented programming courses. Answers to research questions are obtained by mean of a quantitative research. Analysis results showed that all expectations about the system effectiveness are satisfied: 1) almost students agree with the assessment method and feedback given by the system; 2) iterative learning is helpful in improving students' programming skill; 3) and it facilitates students to pay more attention to code quality. This study also points out issues of the current system and propose suggestions to improve system performance. © 2020 ACM.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:style_check', 'feedback:test_output', 'interaction:multiple', 'language:java', 'skill:code_quality', 'skill:correctness', 'skill:maintainability', 'skill:readability', 'technique:unit_testing', 'tool:progedu', 'tool:progedu4group', 'type:evaluation_paper']"
102,rayyan-354359392,Café: Automatic correction and feedback of programming challenges for a CS1 course,"Liénardy S., Leduc L., Verpoorten D., Donnet B.","This paper introduces Café (“Correction Automatique et Feedback des Étudiants”), an on-line platform designed to assess and deliver automatic feedback and feedforward information to CS1 students, both on process and products of series of programming exercises, targeting especially an informal Loop Invariant for building the code. The paper reports on the first trials of Café with a group of 80 students. Results show that Café is used, usable, and appreciated by students. © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7686-0/20/02...$15.00",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:dsl_rules', 'feedback:personalised_feedback', 'interaction:multiple', 'language:C', 'skill:correctness', 'technique:dsl_rules', 'technique:test_cases', 'technique:unit_testing', 'tool:cafe', 'type:description', 'type:evaluation']"
103,rayyan-354359393,Analysing hint based problem solving strategy among novice programmers through gamification technique,"Jannat F., Sazki A.C., Bashar S.B.","All programming and computer science-related courses require problem-solving skills and abilities. Problem-solving skill is one of the most demanding skills in this area. On the other hand, it is also highly noticeable that novice programmers and new programmers face huge amounts of difficulties and obstacles when it comes to solving problems. This research paper analyses the common behaviours and patterns that new programmers make when they tackle problems. At the same time, this research work proposed a noble hint based gamification method for novice programmers to better understand their abilities and improve their skills in a more engaging way. The proposed method had been evaluated with some novice programmers in two-way testing. We also compared the new method with the conventional method in the field level. The evaluation analysis of this work showed that the participants exposed in the proposed method did better than the participants exposed in conventional method. © ECGBL 2020.All right reserved.",2021,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:gamified', 'feedback:program_repair', 'feedback:static_analysis', 'interaction:unknown', 'language:python', 'skill:code_quality', 'technique:program_repair', 'technique:static_analysis', 'tool:pylint', 'type:description']"
104,rayyan-354359394,RefacTutor: An Interactive Tutoring System for Software Refactoring,"Haendler T., Neumann G., Smirnov F.","While software refactoring is considered important to manage software complexity, it is often perceived as difficult and risky by software developers and thus neglected in practice. In this article, we present refacTutor, an interactive tutoring system for promoting software developers’ practical competences in software refactoring. The tutoring system provides immediate feedback to the users regarding the quality of the software design and the functional correctness of the (modified) source code. In particular, after each code modification (refactoring step), the user can review the results of run-time regression tests and compare the actual software design (as-is) with the targeted design (to-be) in order to check quality improvement. For this purpose, structural and behavioral diagrams of the Unified Modeling Language (UML2) representing the as-is software design are automatically reverse-engineered from source code. The to-be UML design diagrams can be pre-specified by the instructor. To demonstrate the technical feasibility of the approach, we provide a browser-based software prototype in Java accompanied by a collection of exercise examples. Moreover, we specify a viewpoint model for software refactoring, allocate exercises to competence levels and describe an exemplary path for teaching and training. © 2020, Springer Nature Switzerland AG.",2019,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:diagram_generation', 'feedback:static_analysis', 'feedback:unit_testing', 'interaction:unknown', 'language:java', 'skill:code_quality', 'skill:correctness', 'skill:refactoring', 'technique:test_cases', 'technique:uml', 'tool:refactutor', 'type:description']"
105,rayyan-354359395,Enhancing a theory-focused course through the introduction of automatically assessed programming exercises - lessons learned,"Soll M., Kobras L., Johannsen M., Biemann C.","In this paper, we describe our lessons learned during the introduction of automatically assessed programming exercises to a Bachelor's level course on algorithms and data structures in the Winter semester 2019/2020, which is yearly taken by around 300 students. The course used to mostly focus on theoretical and formal aspects of selected algorithms and data structures. While still maintaining the primary focus of a theoretical computer science course, we introduce a secondary objective of enhancing programming competence by giving practical programming exercises based on select topics from the course. With these assignments, the students should improve their understanding of the theoretical aspects as well as their programming skills. The programming assignments were given in regular intervals during lecture period with a thematic alignment between assignments and lectures. To compensate for the new set of tasks, the workload of assignments on theoretical aspect was reduced. We describe the different experiences and lessons learned through the introduction and conduction of these exercises. A user study with 44 participants shows that the introduction was perceived well by the students, although improvements are still possible, especially in the area of feedback to the students. © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2020,"['approach:fully_automated', 'data:bespoke_assignment', 'data_available:False', 'evaluation:student_survey', 'evaluation:user_study', 'feedback:test_output', 'interaction:multiple', 'language:java', 'language:python', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:coderunner', 'type:experience_report']"
106,rayyan-354359396,Automatic feedback provision in teaching computational science,"Fangohr H., O’Brien N., Hovorka O., Kluyver T., Hale N., Prabhakar A., Kashyap A.","We describe a method of automatic feedback provision for students learning computational science and data science methods in Python. We have implemented, used and refined this system since 2009 for growing student numbers, and summarise the design and experience of using it. The core idea is to use a unit testing framework: the teacher creates a set of unit tests, and the student code is tested by running these tests. With our implementation, students typically submit work for assessment, and receive feedback by email within a few minutes after submission. The choice of tests and the reporting back to the student is chosen to optimise the educational value for the students. The system very significantly reduces the staff time required to establish whether a student’s solution is correct, and shifts the emphasis of computing laboratory student contact time from assessing correctness to providing guidance. The self-paced nature of the automatic feedback provision supports a student-centred learning approach. Students can re-submit their work repeatedly and iteratively improve their solution, and enjoy using the system. We include an evaluation of the system from using it in a class of 425 students. © Springer Nature Switzerland AG 2020.",2020,"['approach:fully_automated', 'data:internal_assignments', 'data_available:True', 'evaluation:analytics', 'evaluation:student_survey', 'feedback:style_check', 'feedback:unit_testing', 'interaction:multiple', 'language:python', 'skill:correctness', 'skill:readability', 'technique:test_cases', 'tool:afps', 'type:description', 'type:evaluation']"
107,rayyan-354359397,Towards Answering “Am I on the Right Track?” Automatically using Program Synthesis,"Feldman M.Q., Wang Y., Byrd W.E., Guimbretière F., Andersen E.","Students learning to program often need help completing assignments and understanding why their code does not work as they expect it to. One common place where they seek such help is at teaching assistant office hours. We found that teaching assistants in introductory programming (CS1) courses frequently answer some variant of the question “Am I on the Right Track?”. The goal of this work is to develop an automated tool that provides similar feedback for students in real-time from within an IDE as they are writing their program. Existing automated tools lack the generality that we seek, often assuming a single approach to a problem, using hand-coded error models, or applying sample fixes from other students. In this paper, we explore the use of program synthesis to provide less constrained automated answers to “Am I on the Right Track” (AIORT) questions. We describe an observational study of TA-student interactions that supports targeting AIORT questions, as well as the development of and design considerations behind a prototype integrated development environment (IDE). The IDE uses an existing program synthesis engine to determine if a student is on the right track and we present pilot user studies of its use. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:manual_grading', 'evaluation:student_survey', 'feedback:personalised_feedback', 'feedback:program_repair', 'interaction:multiple', 'language:scheme', 'skill:correctness', 'technique:model_solution_req', 'technique:program_synthesis', 'tool:aiort', 'type:description', 'type:evaluation']"
108,rayyan-354359398,Static analyses in python programming courses,"Liu D., Petersen A.","Students learning to program often rely on feedback from the compiler and from instructor-provided test cases to help them identify errors in their code. This feedback focuses on functional correctness, and the output, which is often phrased in technical language, may be difficult to for novices to understand or effectively use. Static analyses may be effective as a complementary aid, as they can highlight common errors that may be potential sources of problems. In this paper, we introduce PyTA, a wrapper for pylint that provides custom checks for common novice errors as well as improved messages to help students fix the errors that are found. We report on our experience integrating PyTA into an existing online system used to deliver programming exercises to CS1 students and evaluate it by comparing exercise submissions collected from the integrated system to previously collected data. This analysis demonstrates that, for students who chose to read the PyTA output, we observed a decrease in time to solve errors, occurrences of repeated errors, and submissions to complete a programming problem. This suggests that PyTA, and static analyses in general, may help students identify functional issues in their code not highlighted by compiler feedback and that static analysis output may help students more quickly identify debug their code. © 2019 Copyright held by the owner/author(s).",2019,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:analytics', 'evaluation:correlation_grades', 'feedback:personalised_feedback', 'feedback:static_analysis', 'interaction:multiple', 'language:python', 'skill:code_quality', 'technique:static_analysis', 'tool:pcrs', 'tool:pylint', 'tool:pyta', 'type:evaluation_paper']"
109,rayyan-354359401,Automatic diagnosis and correction of logical errors for functional programming assignments,"Lee J., Song D., So S., Oh H.","We present FixML, a system for automatically generating feedback on logical errors in functional programming assignments. As functional languages have been gaining popularity, the number of students enrolling functional programming courses has increased significantly. However, the quality of feedback, in particular for logical errors, is hardly satisfying. To provide personalized feedback on logical errors, we present a new error-correction algorithm for functional languages, which combines statistical error-localization and type-directed program synthesis enhanced with components reduction and search space pruning using symbolic execution. We implemented our algorithm in a tool, called FixML, and evaluated it with 497 students' submissions from 13 exercises, including not only introductory but also more advanced problems. Our experimental results show that our tool effectively corrects various and complex errors: it fixed 43% of the 497 submissions in 5.4 seconds on average and managed to fix a hard-to-find error in a large submission, consisting of 154 lines. We also performed user study with 18 undergraduate students and confirmed that our system actually helps students to better understand their programming errors. © 2018 Copyright held by the owner/author(s).",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:True', 'evaluation:user_study', 'feedback:personalised_feedback', 'feedback:program_repair', 'interaction:unknown', 'language:ocaml', 'skill:correctness', 'technique:code_repair_for_feedback', 'technique:model_solution_req', 'technique:test_cases', 'tool:fixml', 'type:description', 'type:evaluation']"
110,rayyan-354359403,Teaching how to program using automated assessment and functional glossy games (Experience Report),"Almeida J.B., MacEdo N., Proenca J.","Our department has long been an advocate of the functional-first school of programming and has been teaching Haskell as a first language in introductory programming course units for 20 years. Although the functional style is largely beneficial, it needs to be taught in an enthusiastic and captivating way to fight the unusually high computer science drop-out rates and appeal to a heterogeneous population of students. This paper reports our experience of restructuring, over the last 5 years, an introductory laboratory course unit that trains hands-on functional programming concepts and good software development practices. We have been using game programming to keep students motivated, and following a methodology that hinges on test-driven development and continuous bidirectional feedback. We summarise successes and missteps, and how we have learned from our experience to arrive at a model for comprehensive and interactive functional game programming assignments and a general functionally-powered automated assessment platform, that together provide a more engaging learning experience for students. In our experience, we have been able to teach increasingly more advanced functional programming concepts while improving student engagement. © 2018 author(s).",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:anecdotal', 'feedback:gamified', 'interaction:multiple', 'language:Haskell', 'skill:code_quality', 'skill:correctness', 'skill:maintainability', 'skill:readability', 'technique:code_metrics', 'technique:static_analysis', 'technique:style_check', 'technique:test_cases', 'technique:unit_testing', 'tool:haap', 'type:experience_report']"
111,rayyan-354359404,An automated assessment system for analysis of coding convention violations in Java programming assignments*,"Chen H.-M., Chen W.-H., Lee C.-C.","Coding conventions are a set of coding guidelines used by software developers to improve the readability of source code and increase software maintainability. Understanding coding conventions has become an indispensable discipline in software engineering; however, many university-level programming courses fail to prepare their students in this regard. In this study, we examined the distribution of coding convention violations in the assignments of programming courses where coding conventions are neglected. We then developed an automated system by which students can submit their programs and obtain immediate feedback on their coding assignments. Moreover, the system reduces the workload on instructors by providing insight into the quality of the code presented by students. © 2018 Journal of Information Science and Engineering. All Rights Reserved.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:usage', 'feedback:style_check', 'interaction:multiple', 'language:java', 'skill:code_quality', 'skill:correctness', 'skill:readability', 'technique:static_analysis', 'technique:test_cases', 'tool:checkstyle', 'tool:progedu', 'type:description', 'type:evaluation']"
112,rayyan-354359405,Soploon: A virtual assistant to help teachers to detect object-oriented errors in students’ source codes,"Vallejos S., Berdun L.S., Armentano M.G., Soria Á., Teyseyre A.R.","When checking students’ source codes, teachers tend to overlook some errors. This work introduces Soploon, a tool that automatically detects novice programmer errors. By using this tool, teachers can reduce the number of overlooked errors. Thus, students receive a more complete and exhaustive feedback about their errors and misconceptions. © 2018 Wiley Periodicals, Inc.",2018,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'evaluation:manual_grading', 'feedback:static_analysis', 'interaction:single', 'language:java', 'skill:correctness', 'skill:maintainability', 'skill:oop', 'technique:dsl_rules', 'technique:matching_rules', 'tool:soploon', 'type:description', 'type:evaluation']"
113,rayyan-354359406,ViDA: A virtual debugging advisor for supporting learning in computer programming courses,"Lee V.C.S., Yu Y.T., Tang C.M., Wong T.L., Poon C.K.","Many students need assistance in debugging to achieve progress when they learn to write computer programs. Face-to-face interactions with individual students to give feedback on their programs, although definitely effective in facilitating their learning, are becoming difficult to achieve with ever-growing class sizes. This paper proposes a novel approach to providing practical automated debugging advice to support students' learning, based on the strong relationship observed between common wrong outputs and the corresponding common bugs in students' programs. To implement the approach, we designed a generic system architecture and process, and developed a tool called Virtual Debugging Advisor (ViDA) that was put into use in classes in a university. To evaluate the effectiveness of ViDA, a controlled experiment and a survey were conducted with first year engineering students in an introductory computer programming course. Results are encouraging, showing that (a) a higher proportion of students could correct their faulty code themselves with ViDA enabled, (b) an overwhelming majority of respondents found ViDA helpful for their learning of programming, and (c) most respondents would like to keep ViDA enabled when they practice writing programs. © 2018 John Wiley & Sons Ltd",2018,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'evaluation:with_without', 'feedback:test_output', 'interaction:multiple', 'language:language__agnostic', 'skill:correctness', 'technique:test_cases', 'technique:unit_testing', 'tool:pass', 'type:description', 'type:evaluation']"
114,rayyan-354359407,Providing meaningful feedback for autograding of programming assignments,"Haldeman G., Tjang A., Babeş-Vroman M., Bartos S., Shah J., Yucht D., Nguyen T.D.","Autograding systems are increasingly being deployed to meet the challenge of teaching programming at scale. We propose a methodology for extending autograders to provide meaningful feedback for incorrect programs. Our methodology starts with the instructor identifying the concepts and skills important to each programming assignment, designing the assignment, and designing a comprehensive test suite. Tests are then applied to code submissions to learn classes of common errors and produce classifiers to automatically categorize errors in future submissions. The instructor maps the errors to concepts and skills and writes hints to help students find their misconceptions and mistakes. We have applied the methodology to two assignments from our Introduction to Computer Science course. We used submissions from one semester of the class to build classifiers and write hints for observed common errors. We manually validated the automatic error categorization and potential usefulness of the hints using submissions from a second semester. We found that the hints given for erroneous submissions should be helpful for 96% or more of the cases. Based on these promising results, we have deployed our hints and are currently collecting submissions and feedback from students and instructors. © 2018 Association for Computing Machinery.",2018,"['approach:semi_automatic', 'data:internal_assignments', 'data_available:False', 'evaluation:compared_to_human', 'feedback:dsl_rules', 'feedback:hints', 'feedback:unit_testing', 'interaction:unknown', 'language:unknown_language', 'skill:correctness', 'technique:knowledge_base', 'technique:test_cases', 'tool:CSF', 'type:description', 'type:evaluation']"
115,rayyan-354359408,ArTEMiS - An automatic assessment management system for interactive learning,"Krusche S., Seitz A.","The increasing number of students in computer science courses leads to high efforts in manual assessment of exercises. Existing assessment systems are not designed for exercises with immediate feedback in large classes. In this paper, we present an AuTomated assEssment Management System for interactive learning. ArTEMiS assesses solutions to programming exercises automatically and provides instant feedback so that students can iteratively solve the exercise. It is open source and highly scalable based on version control, regression testing and continuous integration. ArTEMiS offers an online code editor with interactive exercise instructions, is programming language independent and applicable to a variety of computer science courses. By using it, students gain experiences in version control, dependency management and continuous integration. We used ArTEMiS in 3 university and 1 online courses and report about our experiences. We figured out that ArTEMiS is suitable for beginners, helps students to realize their progress and to gradually improve their solutions. It reduces the effort of instructors and enhances the learning experience of students. © 2018 Association for Computing Machinery.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:engagement', 'feedback:static_analysis', 'feedback:test_output', 'interaction:multiple', 'language:java', 'skill:correctness', 'technique:ci', 'technique:test_cases', 'technique:unit_testing', 'tool:artemis', 'type:description', 'type:evaluation']"
116,rayyan-354359409,Linkage objects for generalized instruction in coding (logic),"Carmichael T., Blink M.J., Stamper J., Gieske E.","Linkage Objects for Generalized Instruction in Coding (LOGIC) is an intelligent system for online tutoring which detects errors among programming exercises to improve understanding of student progress. This system represents an implementation of the Hint Factory method for automated hint generation. In this approach, variables and their dependencies are abstracted from correct coding solutions to determine all the possible paths towards a solution, regardless of the programming language or variable names. Incomplete programs can be compared to these unique paths after code normalization, and the next best line can be supplied in the form of a hint. Errors are recorded based on discrepancy between best-match and the student's code. The final report categorizing all errors is compiled to benefit the teacher's effectiveness, highlighting common errors made by students. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",2018,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:auto_graders', 'feedback:hints', 'feedback:machine_learning', 'interaction:multiple', 'language:language__agnostic', 'skill:correctness', 'technique:ml', 'technique:pattern_matching', 'tool:cloudcoder', 'tool:logic', 'type:description', 'type:evaluation']"
117,rayyan-354359410,Automated data-driven hints for computer programming students,"Chow S., Yacef K., Koprinska I., Curran J.","Formative feedback is essential for learning computer programming but is also a challenge to automate because of the many solutions a programming exercise can have. Whilst programming tutoring systems can easily generate automated feedback on how correct a program is, they less often provide some personalised guidance on how to improve or fix the code. In this paper, we present an approach for generating hints using previous student data. Utilising a range of techniques such as filtering, clustering and pattern mining, four different types of data-driven hints are generated: input suggestion, code-based, concept and pre-emptive hints. We evaluated our approach with data from 5529 students using the Grok Learning platform for teaching programming in Python. The results show that we can generate various types of hints for over 90% of students with data from only 10 students, and hence, reduce the coldstart problem. © 2017 ACM.",2017,"['approach:fully_automated', 'data:grok', 'data_available:on_request', 'evaluation:analytics', 'feedback:hints', 'feedback:static_analysis', 'feedback:suggest_similar', 'interaction:multiple', 'language:python', 'skill:correctness', 'technique:unit_testing', 'tool:grok', 'type:description', 'type:evaluation']"
118,rayyan-354359411,Learning and teaching numerical methods with a system for automatic assessment,"Jerše G., Lokar M.","The emphasis in several courses at technical facilities is on using a computer to perform numerical methods. Instead of focusing on mathematical rigorousness such courses usually concentrate on demonstrating the practical usage of numerical mathematical methods to the students. The practical usage of numerical methods is best learned by exposing the students to a large set of exercises, which they have tn solve The process of solving problems has to be supervised in order to provide the students with a swift feedback about the quality of their solutions. The following paper presents a web system for automatic assessment called Projekt Tomo, which was developed as a support tool for leaching programming and numerical methods oriented classes. © 2017 Research Information Ltd.",2017,"['approach:fully_automated', 'data:none', 'data_available:False', 'evaluation:none', 'feedback:test_output', 'interaction:multiple', 'language:octave', 'language:python', 'skill:correctness', 'technique:dsl_rules', 'technique:unit_testing', 'tool:project_tomo', 'type:description']"
119,rayyan-354359412,Ask-Elle: an Adaptable Programming Tutor for Haskell Giving Automated Feedback,"Gerdes A., Heeren B., Jeuring J., van Binsbergen L.T.","Ask-Elle is a tutor for learning the higher-order, strongly-typed functional programming language Haskell. It supports the stepwise development of Haskell programs by verifying the correctness of incomplete programs, and by providing hints. Programming exercises are added to Ask-Elle by providing a task description for the exercise, one or more model solutions, and properties that a solution should satisfy. The properties and model solutions can be annotated with feedback messages, and the amount of flexibility that is allowed in student solutions can be adjusted. The main contribution of our work is the design of a tutor that combines (1) the incremental development of different solutions in various forms to a programming exercise with (2) automated feedback and (3) teacher-specified programming exercises, solutions, and properties. The main functionality is obtained by means of strategy-based model tracing and property-based testing. We have tested the feasibility of our approach in several experiments, in which we analyse both intermediate and final student solutions to programming exercises, amongst others. © 2016, International Artificial Intelligence in Education Society.",2016,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:dsl_rules', 'feedback:hints', 'interaction:multiple', 'language:Haskell', 'skill:code_design', 'skill:correctness', 'technique:model_solution_req', 'technique:property_based_testing', 'tool:ask_elle', 'type:description', 'type:evaluation']"
120,rayyan-354359413,An AI system for coaching novice programmers,"Cruz G., Jones J., Morrow M., Gonzalez A., Gooch B.","We inhabit a century where every job will be technical. In the 21st century, learning to program a computer is empowerment. Programming instruction teaches procedural and functional thinking, project management and time management, skills that are essential components of an empowered individual. Programming is the power to create, the power to change and the power to influence. Today’s students regardless of their field of study or need this fundamental knowledge. Rapidly giving students meaningful feedback is a fundamental component of an effective educational experience. A common problem in modern education is scalability, as class size increases an instructor’s ability to provide meaningful feedback decreases. We report on an online Artificial Intelligence (AI) system capable of providing insightful narrative based coaching to beginning programmers. We document system tests to ensure that: it generates a unique response to every input, makes responses in real time, and is deployable online. © Springer International Publishing AG 2017.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:code_metrics', 'feedback:personalised_feedback', 'feedback:unit_testing', 'interaction:unknown', 'language:java', 'skill:code_quality', 'technique:metrics', 'technique:ml', 'technique:pre_defined_questions', 'technique:static_analysis', 'tool:checkstyle', 'tool:pmd', 'type:description', 'type:evaluation']"
121,rayyan-354359414,Automatic extraction of AST patterns for debugging student programs,"Lazar T., Možina M., Bratko I.","When implementing a programming tutor it is often difficult to manually consider all possible errors encountered by students. An alternative is to automatically learn a bug library of erroneous patterns from students’ programs. We propose abstract-syntax-tree (AST) patterns as features for learning rules to distinguish between correct and incorrect programs. We use these rules to debug student programs: rules for incorrect programs (buggy rules) indicate mistakes, whereas rules for correct programs group programs with the same solution strategy. To generate hints, we first check buggy rules and point out incorrect patterns. If no buggy rule matches, we use rules for correct programs to recognize the student’s intent and suggest missing patterns. We evaluate our approach on past student programming data for a number of Prolog problems. For 31 out of 44 problems, the induced rules correctly classify over 85% of programs based only on their structural features. For approximately 73% of incorrect submissions, we are able to generate hints that were implemented by the student in some subsequent submission. © Springer International Publishing AG 2017.",2017,"['approach:fully_automated', 'data:internal_assignments', 'data_available:False', 'evaluation:manual_grading', 'feedback:hints', 'feedback:machine_learning', 'feedback:suggest_similar', 'interaction:multiple', 'language:prolog', 'skill:correctness', 'technique:pattern_matching', 'tool:codq', 'type:description', 'type:evaluation']"
122,rayyan-354359415,Self- and automated assessment in programming MOOCs,"Lepp M., Luik P., Palts T., Papli K., Suviste R., Säde M., Hollo K., Vaherpuu V., Tånisson E.","This paper addresses two MOOCs in Estonian about programming where different kinds of assessment were used. We have used two kinds of automated assessment: quizzes in Moodle and programming exercises with automated feedback provided by Moodle plug-in VPL. We also used two kinds of self-assessment: (1) self-assessment questions with feedback and explanations for every answer and (2) so-called “troubleshooters” for every programming exercise, which contain answers to the questions that can arise during the solution of a given exercise. This paper describes our experience in the creation of quizzes, programming exercises, and tests for automated feedback, self-assessment questions, and troubleshooters. The paper discusses the problems and questions that arose during this process and presents learners’ opinions about self- and automated assessment. The paper concludes with a discussion of the impact of self- and automated assessment in MOOCs, describes the work of MOOC organizers and the behaviour of learners in MOOCs. © Springer International Publishing AG 2017.",2016,"['approach:unclear', 'data:internal_assignments', 'data_available:False', 'evaluation:student_survey', 'feedback:test_output', 'interaction:unknown', 'language:python', 'skill:correctness', 'technique:unit_testing', 'tool:vpl', 'type:evaluation_paper']"
